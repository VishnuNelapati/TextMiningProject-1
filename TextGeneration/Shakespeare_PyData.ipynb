{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Shakespeare PyData.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hofsF4rCsd8k",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dimtr/PyDataEHV_workshop/blob/master/TextGeneration/Shakespeare_PyData.ipynb\" target=\"_parent\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIkKrYoVSIcI",
        "colab_type": "text"
      },
      "source": [
        "# Text Generation with Recurrent Neural Networks\n",
        "In this workshop, we see how recurrent Neural Networks could be used as Generative Models. They can learn the sequences of a problem and generate entirely new plausible sequences for the problem domain.\n",
        "\n",
        "We will discover how to create a simple text generation model using Python in [Keras](https://keras.io/) that generates text, word-by-word. We will work with the dataset of Shakespeare's writing Andrej Karpathy's [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).\n",
        "\n",
        "Given a sequence of words, the model trained on our dataset will predict the next most probable word. We will call the model repeatedly to generate longer sequences.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYTDXoAXXQaH",
        "colab_type": "text"
      },
      "source": [
        "##Setup\n",
        "\n",
        "####Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPl_k7f8qANu",
        "colab_type": "code",
        "outputId": "665774dc-7b28-42c5-8a86-1620fc74c0d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW8XEFpPaG6t",
        "colab_type": "text"
      },
      "source": [
        "####Import Keras and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ENSoJKsaRGM",
        "colab_type": "code",
        "outputId": "64603e9e-9097-4cd5-c913-8e2aff736980",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout,SimpleRNN, LSTM, Embedding\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import LambdaCallback, EarlyStopping\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.utils import shuffle\n",
        "from glob import glob\n",
        "\n",
        "import numpy as np\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Otr5uy-y7rs",
        "colab_type": "text"
      },
      "source": [
        "#### Clone GitHub Repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLXniWMUxmNs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "c0633ab1-1839-4494-8b59-06367d7ec015"
      },
      "source": [
        "!git clone https://github.com/dimtr/PyDataEHV_workshop/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'PyDataEHV_workshop'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects: 100% (134/134), done.\u001b[K\n",
            "remote: Compressing objects: 100% (102/102), done.\u001b[K\n",
            "remote: Total 250 (delta 72), reused 89 (delta 32), pack-reused 116\u001b[K\n",
            "Receiving objects: 100% (250/250), 297.65 MiB | 14.37 MiB/s, done.\n",
            "Resolving deltas: 100% (95/95), done.\n",
            "Checking out files: 100% (77/77), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1SCfRbkb2Fw",
        "colab_type": "text"
      },
      "source": [
        "####Reading the data\n",
        "\n",
        "We load the Shakespeare text file and take a look at a part of the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0s5PSeNDtHkl",
        "colab_type": "code",
        "outputId": "9aeda9ad-ab2f-4e28-aa12-749ebfff3156",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "with open('/content/PyDataEHV_workshop/TextGeneration/datasets/shakespeare/shakespeare.txt', encoding='utf-8') as f:\n",
        "   story = f.readlines()\n",
        "   \n",
        "print(''.join(story[:20]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0T95x2Z3ftl",
        "colab_type": "text"
      },
      "source": [
        "##Process the Text\n",
        "Before training the model, we need to process the text in a form that is interpretable by the model.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHCR_I-Web-j",
        "colab_type": "text"
      },
      "source": [
        "#### Story in words\n",
        "\n",
        "First, we convert the story data into chunks of words or tokens. Since we want our model to also recognize puntuations as words, we use [replace()](https://docs.python.org/2/library/string.html#string.replace) to add white spaces around them and then use [split()](https://docs.python.org/2/library/stdtypes.html#str.split) to split the story into word chunks. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEz83f1rcm7c",
        "colab_type": "code",
        "outputId": "b4c25fd7-59ac-46c5-b8ea-156f6c4dee95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "story_in_words = []\n",
        "for i, line in enumerate(story):\n",
        "  story[i] = line.lower().replace('.', ' . ').\\\n",
        "                          replace(',', ' , ').\\\n",
        "                          replace('?', ' ? ').\\\n",
        "                          replace('\"', ' \" ').\\\n",
        "                          replace('!', ' ! ').\\\n",
        "                          replace(':', ' : ').\\\n",
        "                          replace(';', ' ; ').\\\n",
        "                          replace('--', ' ').\\\n",
        "                          replace('-', ' ').\\\n",
        "                          replace(',', ' , ')\n",
        "  story_in_words.extend(story[i].split(' '))\n",
        "\n",
        "print(\"Total number of tokens in story: %d\" % len(story_in_words))\n",
        "print(\"Unique words in tokens: %d\" %len(set(story_in_words)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of tokens in story: 475963\n",
            "Unique words in tokens: 14833\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtYVe_amqAOP",
        "colab_type": "text"
      },
      "source": [
        "####Creating sequences\n",
        "\n",
        "The next step is to split the entire text into sequences of a certain length. We specify this length by using the *hyper parameter* SEQ_LEN. Sequence length is the number of words that the generative model would take as input to predict the next word. \n",
        "\n",
        "We go over the story by, shifting by one word at each step and take SEQ_LEN + 1 words in a sequence at a time.\n",
        "\n",
        "\n",
        "\n",
        "NOTE: You can play around with different values of the SEQ_LEN hyper parameter to investigate how it affects the performance of your model. This method of trying out different values of hyper parameters in order to find an optimal set for a learning algorithm is called *Hyperparameter Tuning*. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SNEyBCWqwgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEQ_LEN = 10\n",
        "step = 1\n",
        "sentences = []\n",
        "for i in range(0, len(story_in_words) - SEQ_LEN, step):\n",
        "  sentences.append(story_in_words[i : i + SEQ_LEN + 1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UnyuPcQqxcc",
        "colab_type": "text"
      },
      "source": [
        "####Tokenizer\n",
        "\n",
        "Now we need to map the strings to numeric representation. We want each unique word to be represented by a unique integer number. We use the [Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) class by Keras for this task. The class then calls a function that fits the tokenizer on the our sequences of text and builds an internal vocabulary. \n",
        "\n",
        "Note: 0 is a reserved index in this class that won't be assigned to any word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKKGz-x0qR3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)  # transforms each text sequence into a sequence of integer, where each integer represents a unique word\n",
        "sequences = np.asarray(sequences)                    # converting a list to a numpy array\n",
        "vocab = tokenizer.word_counts                        # Dict object of vocabulary with frequency count\n",
        "\n",
        "VOCAB_LEN = len(tokenizer.word_counts) + 1\n",
        "total = sequences.shape[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwGJ7BD_dDMF",
        "colab_type": "text"
      },
      "source": [
        "We take an example of a sentance and see how the Tokenizer class transforms it into a sequence of integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt5WuYNCbVdC",
        "colab_type": "code",
        "outputId": "295f0841-02de-4949-d2f8-06d36a81a634",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "temp = 'You are all resolved rather to die than to famish ?'\n",
        "\n",
        "tokens = tokenizer.texts_to_sequences([temp.split()])\n",
        "print(\"Numeric representation of the sentence: 'You are all resolved rather to die than to famish ?' is --> \", tokens)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numeric representation of the sentence: 'You are all resolved rather to die than to famish ?' is -->  [[13, 48, 41, 1535, 378, 8, 198, 71, 8, 3461, 16]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plEF3tEv0I3f",
        "colab_type": "text"
      },
      "source": [
        "####Training and Test data split\n",
        "\n",
        "Data is shuffled and split into training and test dataset with 15% of the data in the test split. \n",
        "\n",
        "The first *SEQ_LEN* words of the sequence make the input data and the remaining last word is the target data or the next probable word that the model should output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BiZmqykxBLs",
        "colab_type": "code",
        "outputId": "109986eb-ec13-4bde-d3fc-ada048a5d8e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "sequences = shuffle(sequences)\n",
        "\n",
        "train_input = sequences[int(total * 0.15):, :-1]\n",
        "train_output = sequences[int(total * 0.15):, -1]\n",
        "\n",
        "test_input = sequences[:int(total * 0.15), :-1]\n",
        "test_output = sequences[:int(total * 0.15), -1]\n",
        "\n",
        "\n",
        "print(\"Input Training Data Shape:\", train_input.shape)\n",
        "print(\"Target Training Data Shape:\", train_output.shape)\n",
        "\n",
        "print(\"Input Test Data Shape:\", test_input.shape)\n",
        "print(\"Target Test Data Shape:\", test_output.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Training Data Shape: (404561, 10)\n",
            "Target Training Data Shape: (404561,)\n",
            "Input Test Data Shape: (71392, 10)\n",
            "Target Test Data Shape: (71392,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVDVyfD00aqt",
        "colab_type": "text"
      },
      "source": [
        "####Build the RNN Model\n",
        "\n",
        "We use the [Keras Sequential Model](https://keras.io/getting-started/sequential-model-guide/) to define the model. To build our simple RNN text generation model, we use four keras layers:\n",
        "\n",
        "\n",
        "*   [keras.layers.Embedding](https://keras.io/layers/embeddings/): used to train a dense representation of words and their relative meanings.\n",
        "*   [keras.layers.SimpleRNN](https://keras.io/layers/recurrent/#simplernn): fully connected RNN layer where the output is fed back to the input.\n",
        "*   [keras.layers.Dropout](https://keras.io/layers/core/#dropout): applies a regularization technique where randomly selected neurons are ignored or \"dropped-out\" during training.\n",
        "*   [keras.layers.Dense](https://keras.io/layers/core/#dense): regular densely connected neural network layer with output size equal to the vocabulary size (number of unique words). This layer is added at the end and uses the softmax activation to output the probablities (that add up to one) for each word. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PMc52m3Afhg",
        "colab_type": "code",
        "outputId": "6f9d2b4b-2f16-49b5-99e0-f627b9f18dd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(VOCAB_LEN, 256, input_length=SEQ_LEN))\n",
        "model.add(SimpleRNN(128, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(SimpleRNN(64))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(VOCAB_LEN, activation='softmax'))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fci7X5txCaKA",
        "colab_type": "text"
      },
      "source": [
        "We compile the model to configure it for training and use categorical crossentropy as our loss function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkNBch7EAi1g",
        "colab_type": "code",
        "outputId": "60f4d6a1-9ff5-49e3-d1fa-cde0e6c34c30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 10, 256)           3797504   \n",
            "_________________________________________________________________\n",
            "simple_rnn_1 (SimpleRNN)     (None, 10, 128)           49280     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 10, 128)           0         \n",
            "_________________________________________________________________\n",
            "simple_rnn_2 (SimpleRNN)     (None, 64)                12352     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 14834)             964210    \n",
            "=================================================================\n",
            "Total params: 4,823,346\n",
            "Trainable params: 4,823,346\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2c9sr7U0p1w",
        "colab_type": "text"
      },
      "source": [
        "###Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hOylH2UDxXR",
        "colab_type": "text"
      },
      "source": [
        "We create a generator function that generates data batch-by-batch. The generator is run in parallel to the model, for efficiency. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAMVl9Bexap2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator(sent, word, batch_size):\n",
        "\n",
        "  global index\n",
        "  index = 0\n",
        "  while True:\n",
        "    x = np.zeros((batch_size, SEQ_LEN), dtype=np.int)\n",
        "    y = np.zeros((batch_size, VOCAB_LEN), dtype=np.bool)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "      x[i] = sent[index % len(sent)]\n",
        "      y[i] = to_categorical(word[index % len(word)], num_classes=VOCAB_LEN) #convert integers to one-hot encoded vectors\n",
        "      index = index + 1\n",
        "    yield x,y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0vz5ARoEaxT",
        "colab_type": "text"
      },
      "source": [
        "This function samples an index from a softmax probablity array based on the temperature. This technique is called temperature sampling and is used to improve the quality of samples from language models.\n",
        "\n",
        "Note: The high temperature sample displays greater linguistic variety, but the low temperature sample is more grammatically correct. Lowering the temperature allows you to focus on higher probability output sequences and smooth over deficiencies of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoYihS3CAqru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.flip(np.argsort(probas))[0] ##np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBLJ2ljjGbp-",
        "colab_type": "text"
      },
      "source": [
        "We make a simple display function that prints the stories "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEgcYSEjKqV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display(story):\n",
        "  full = ''\n",
        "  for word in story:\n",
        "    if word == '|newline|':\n",
        "      full = full + '\\n'\n",
        "    elif word in ',.;:?!':\n",
        "      full = full + word\n",
        "    else:\n",
        "      full = full + ' ' + word\n",
        "  \n",
        "  print(full)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIcVgS7U0u1p",
        "colab_type": "text"
      },
      "source": [
        "####Configure Checkpoints\n",
        "\n",
        "We use two types of checkpoints for our model:\n",
        "\n",
        "\n",
        "*   [ModelCheckpoint](https://keras.io/callbacks/#modelcheckpoint): to ensure that checkpoints are saved during training by monitoring a quality (validation accuracy in this case)\n",
        "*   [EarlyStopping](https://keras.io/callbacks/#EarlyStopping): stops training when the monitored quality (validation accuracy) has not improved for a certain number of epochs. This threshold is set by the *patience* argument.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qCZ5JGuAwCx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE =256\n",
        "file_path = \"/content/PyDataEHV_workshop/TextGeneration/checkpoints/Shakespeare/Shakespeare-RNN-epoch{epoch:03d}-words%d-sequence%d-batchsize%d-\" \\\n",
        "            \"loss{loss:.4f}-acc{acc:.4f}-val_loss{val_loss:.4f}-val_acc{val_acc:.4f}.hdf5\" % \\\n",
        "            (VOCAB_LEN, SEQ_LEN, BATCH_SIZE)\n",
        "\n",
        "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', save_best_only=True) # latest best model according to the val_acc monitored will not be overwritten\n",
        "early_stopping = EarlyStopping(monitor='val_acc', patience=10)\n",
        "callbacks_list = [checkpoint, early_stopping]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlEajkmjxwv9",
        "colab_type": "text"
      },
      "source": [
        "Uncomment if you do not want to train the model from scratch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1tgi7MZoGn7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model.load_weights('/content/PyDataEHV_workshop/TextGeneration/checkpoints/Shakespeare/Shakespeare-RNN-epoch008-words14834-sequence10-batchsize256-loss3.6602-acc0.3875-val_loss4.1272-val_acc0.3578.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tADs8aV8025R",
        "colab_type": "text"
      },
      "source": [
        "####Train the Model\n",
        "\n",
        "The model is trained for 30 epochs, but training could stop early due to the callback. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9m5tcGD2A01J",
        "colab_type": "code",
        "outputId": "8de902be-3953-4572-f955-121d81ab9f45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        }
      },
      "source": [
        "model.fit_generator(generator(train_input, train_output, BATCH_SIZE),\n",
        "                    steps_per_epoch=int(len(train_input)/BATCH_SIZE) + 1,\n",
        "                    epochs=30,\n",
        "                    callbacks=callbacks_list,\n",
        "                    validation_data=generator(test_input, test_output, BATCH_SIZE),\n",
        "                    validation_steps=int(len(test_input)/BATCH_SIZE) + 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1142/1142 [==============================] - 37s 32ms/step - loss: 3.5827 - acc: 0.3941 - val_loss: 4.1913 - val_acc: 0.3542\n",
            "Epoch 2/30\n",
            "1142/1142 [==============================] - 37s 32ms/step - loss: 3.5333 - acc: 0.3987 - val_loss: 4.1584 - val_acc: 0.3596\n",
            "Epoch 3/30\n",
            "1142/1142 [==============================] - 36s 31ms/step - loss: 3.4851 - acc: 0.4022 - val_loss: 4.1802 - val_acc: 0.3553\n",
            "Epoch 4/30\n",
            "1142/1142 [==============================] - 36s 31ms/step - loss: 3.4358 - acc: 0.4056 - val_loss: 4.2030 - val_acc: 0.3555\n",
            "Epoch 5/30\n",
            "1142/1142 [==============================] - 36s 31ms/step - loss: 3.3906 - acc: 0.4095 - val_loss: 4.2083 - val_acc: 0.3566\n",
            "Epoch 6/30\n",
            "1142/1142 [==============================] - 36s 31ms/step - loss: 3.3481 - acc: 0.4133 - val_loss: 4.2383 - val_acc: 0.3540\n",
            "Epoch 7/30\n",
            "1142/1142 [==============================] - 36s 32ms/step - loss: 3.3062 - acc: 0.4169 - val_loss: 4.2633 - val_acc: 0.3541\n",
            "Epoch 8/30\n",
            "1142/1142 [==============================] - 36s 31ms/step - loss: 3.2663 - acc: 0.4202 - val_loss: 4.2644 - val_acc: 0.3528\n",
            "Epoch 9/30\n",
            "1142/1142 [==============================] - 36s 31ms/step - loss: 3.2333 - acc: 0.4233 - val_loss: 4.2928 - val_acc: 0.3513\n",
            "Epoch 10/30\n",
            "1142/1142 [==============================] - 35s 31ms/step - loss: 3.1983 - acc: 0.4263 - val_loss: 4.3152 - val_acc: 0.3484\n",
            "Epoch 11/30\n",
            "1142/1142 [==============================] - 36s 31ms/step - loss: 3.1661 - acc: 0.4297 - val_loss: 4.3235 - val_acc: 0.3481\n",
            "Epoch 12/30\n",
            "1142/1142 [==============================] - 35s 31ms/step - loss: 3.1314 - acc: 0.4333 - val_loss: 4.3664 - val_acc: 0.3485\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3e303273c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N69K3rsr08qE",
        "colab_type": "text"
      },
      "source": [
        "###Generate Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIiRYkS-1XiM",
        "colab_type": "text"
      },
      "source": [
        "####Restore the latest checkpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vy6b74c1Ihz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "df2c4686-181a-4cbd-831c-d0c8ecc8cf1c"
      },
      "source": [
        "model.load_weights('/content/PyDataEHV_workshop/TextGeneration/checkpoints/Shakespeare/Shakespeare-RNN-epoch008-words14834-sequence10-batchsize256-loss3.6602-acc0.3875-val_loss4.1272-val_acc0.3578.hdf5')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qCTRit71AOg",
        "colab_type": "text"
      },
      "source": [
        "####Conditional Samples\n",
        "\n",
        "Here, the story is generated word-by-word, based on a prompt provided. If the number of words in the prompt exceed SEQ_LEN, then the prompt is truncated from the beginning to fit the sequence length. If the length of prompt is less than SEQ_LEN, then zeros are padded in the beginning. The whole story is genrated by predicting the next probable word in a loop.\n",
        "\n",
        "Note: Since 0 is used to pad sequences, it is important that the Tokenizer does not use 0 as an index. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Lc3_y34AtyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_cond_samples(no_of_words, temp):\n",
        "    \n",
        "    print('Enter prompt: \\n')\n",
        "    seed = input()\n",
        "    sentence = tokenizer.texts_to_sequences([seed])[0]\n",
        "\n",
        "    sentence = list(pad_sequences([sentence], maxlen=SEQ_LEN, padding='pre', truncating='pre')[0])\n",
        "\n",
        "\n",
        "    gen_story = []\n",
        "    gen_story.extend(seed.split())\n",
        "\n",
        "    for i in range(no_of_words):\n",
        "        x_pred = np.expand_dims(sentence, axis=0) \n",
        "            \n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_indices = sample(preds, temp)\n",
        "\n",
        "        for ix in next_indices:\n",
        "          if ix == 0:\n",
        "            continue\n",
        "          else:\n",
        "            next_word = tokenizer.index_word[ix]\n",
        "            sentence = sentence[1:]  ## removing the least frequent word from the input for the next predictiom\n",
        "            sentence.append(ix)  ## adds the predicted word in the input for the next prediction\n",
        "            break\n",
        "\n",
        "        gen_story.append(next_word)\n",
        "        \n",
        "    display(gen_story)\n",
        "        \n",
        "       \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISZfU2osSEvJ",
        "colab_type": "code",
        "outputId": "fafb87d2-8fd3-4e66-d553-4578b379ac44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "generate_cond_samples(no_of_words = 500, temp=0.5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter prompt: \n",
            "\n",
            "King: what are you doing here Sybil?\n",
            " King: what are you doing here Sybil?. \n",
            " \n",
            " first citizen: \n",
            " if i do not, he should be at home. \n",
            " \n",
            " gloucester: \n",
            " what, this is all to die. \n",
            " \n",
            " gloucester: \n",
            " i will not enter, and take a fire; \n",
            " and i will have a woman's proud rebel, \n",
            " and therefore i mean, and thou shalt not be so. \n",
            " \n",
            " duke vincentio: \n",
            " o, i am going to save those that have his government: \n",
            " the time of the father of his life, \n",
            " that would be gone, my lord, sir, i am sure, and not a word. \n",
            " \n",
            " sicinius: \n",
            " nay, father, i will not be so. \n",
            " \n",
            " biondello: \n",
            " if you do yield. \n",
            " \n",
            " king henry vi: \n",
            " good night, and go, now, my lord, my lords, i am not to your love. \n",
            " \n",
            " king richard ii: \n",
            " my lord, my lord, sir? \n",
            " \n",
            " duke vincentio: \n",
            " what is a man, my lord, i know not not a word: \n",
            " to all the world, by your fortune! \n",
            " \n",
            " second servingman: \n",
            " what, sir, i have heard you speak. \n",
            " \n",
            " romeo: \n",
            " o, to say it, i see your leave. \n",
            " \n",
            " gloucester: \n",
            " my lord, if thou art not. \n",
            " \n",
            " leontes: \n",
            " o, you are too much of your. \n",
            " \n",
            " king richard ii: \n",
            " ay, sir, and not be so. \n",
            " \n",
            " autolycus: \n",
            " so, i have rather? \n",
            " \n",
            " coriolanus: \n",
            " i am my man. \n",
            " \n",
            " duke vincentio: \n",
            " 'tis gone to be so far\n",
            " to make her leave. \n",
            " \n",
            " richard: \n",
            " i have not no more. now, sir, is not your gracious lord, \n",
            " and the sun like wrinkle to their wars. \n",
            " \n",
            " lady anne: \n",
            " he is the duke of hereford, sir, i do know you? \n",
            " \n",
            " aufidius: \n",
            " what,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjTYRruE1C6k",
        "colab_type": "text"
      },
      "source": [
        "####Unconditional Samples\n",
        "\n",
        "Here, the story is generated word-by-word by starting with a random seed. One integer is randomly sampled from the index of words and story is generated by prediciting the next probable work in a loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFs9KdtS0Zjs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_uncond_samples(no_of_words, temp):\n",
        "    np.random.seed(0)\n",
        "    seed = np.random.randint(1, VOCAB_LEN, size=SEQ_LEN)\n",
        "\n",
        "    sentence = seed\n",
        "\n",
        "    sentence = list(pad_sequences([sentence], maxlen=SEQ_LEN, padding='pre')[0])\n",
        "\n",
        "    gen_story = []\n",
        "\n",
        "    gen_story.extend(tokenizer.index_word[w] for w in seed)\n",
        "\n",
        "    for i in range(no_of_words):\n",
        "        x_pred = np.expand_dims(sentence, axis=0)\n",
        "\n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_indices = sample(preds, temp)\n",
        "\n",
        "        for ix in next_indices:\n",
        "            if ix == 0:\n",
        "                continue\n",
        "\n",
        "            else:\n",
        "                next_word = tokenizer.index_word[ix]\n",
        "                sentence = sentence[1:]\n",
        "                sentence.append(ix)\n",
        "                break\n",
        "\n",
        "\n",
        "        gen_story.append(next_word)\n",
        "\n",
        "    display(gen_story)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRZkP_XDPR2c",
        "colab_type": "code",
        "outputId": "46c1baf9-4937-4288-c70b-0499444d4444",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "generate_uncond_samples(no_of_words = 500, temp=0.5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " conclude idles compromise\n",
            " watery edge\n",
            " greets presumes majestical fidiused magistrate. \n",
            " where is the law, and bid his life. \n",
            " \n",
            " gloucester: \n",
            " i will not well the king, \n",
            " and with the man of all a company. \n",
            " \n",
            " first murderer: \n",
            " what, as i see, though i should die: \n",
            " i do not hear, my son, the measure of a noble. \n",
            " \n",
            " king richard iii: \n",
            " i have been a suitor to the king, \n",
            " and not not not to my daughter? \n",
            " \n",
            " miranda: \n",
            " i am a king, and each to be a thief, to the one of the king: \n",
            " but i will be a man? \n",
            " \n",
            " menenius: \n",
            " i am a man, and all her well? \n",
            " \n",
            " first citizen: \n",
            " i am so? \n",
            " \n",
            " second servingman: \n",
            " my lord, i do not be a woman. \n",
            " \n",
            " king richard iii: \n",
            " why, sir, my lord! \n",
            " \n",
            " leontes: \n",
            " i pray you, sir! \n",
            " \n",
            " volumnia: \n",
            " i have a poor man's life. who shall be so? \n",
            " \n",
            " juliet: \n",
            " how now, my lord, my lord, sir, the lord. \n",
            " \n",
            " leontes: \n",
            " i am a good gentleman. \n",
            " \n",
            " aufidius: \n",
            " o, sir? \n",
            " \n",
            " leontes: \n",
            " o, sir? \n",
            " \n",
            " gonzalo: \n",
            " i have no more more than the fool. \n",
            " that i am all to have a language. \n",
            " \n",
            " menenius: \n",
            " i have a very heir. \n",
            " \n",
            " menenius: \n",
            " i saw the crown; \n",
            " and yet thou camest to this that lawless minister\n",
            " with their country's strength, \n",
            " and therefore he made me for you? \n",
            " \n",
            " menenius: \n",
            " 'tis this a brother. \n",
            " \n",
            " gloucester: \n",
            " the death of york! the king of england? \n",
            " \n",
            " king richard iii: \n",
            " i will not what a thing, my lord, thou hast some more? \n",
            " \n",
            " leontes: \n",
            " to morrow, no, sir, my lord, we know, and bid me go. \n",
            " \n",
            " clarence: \n",
            " why, by this time, sir. \n",
            " \n",
            " menenius: \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiFZx-rBTEPR",
        "colab_type": "text"
      },
      "source": [
        "Looking at the generated text, you'll see the model knows when to use punctuations, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCEcBb9hZOPj",
        "colab_type": "text"
      },
      "source": [
        "###Build an LSTM Model\n",
        "\n",
        "We keep the structure for the model similar but use the Keras LSTM layer instead of the SimpleRNN layer. We again use the [Keras Sequential Model](https://keras.io/getting-started/sequential-model-guide/) to define the LSTM based generative model by using four keras layers:\n",
        "\n",
        "\n",
        "*   [keras.layers.Embedding](https://keras.io/layers/embeddings/): used to train a dense representation of words and their relative meanings.\n",
        "*   [keras.layers.LSTM](https://keras.io/layers/recurrent/#lstm): long-short term memory layer composed of a *cell*, an *input gate*, an *output gate* and a *forget gate*.\n",
        "*   [keras.layers.Dropout](https://keras.io/layers/core/#dropout): applies a regularization technique where randomly selected neurons are ignored or \"dropped-out\" during training.\n",
        "*   [keras.layers.Dense](https://keras.io/layers/core/#dense): regular densely connected neural network layer with output size equal to the vocabulary size (number of unique words). This layer is added at the end and uses the softmax activation to output the probablities (that add up to one) for each word. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuF2hSk_2p8d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_model = Sequential()\n",
        "lstm_model.add(Embedding(VOCAB_LEN, 256, input_length=SEQ_LEN))\n",
        "lstm_model.add(LSTM(128, return_sequences=True))\n",
        "lstm_model.add(Dropout(0.2))\n",
        "lstm_model.add(LSTM(64))\n",
        "lstm_model.add(Dropout(0.2))\n",
        "lstm_model.add(Dense(VOCAB_LEN, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m70vEiAEg5tc",
        "colab_type": "text"
      },
      "source": [
        "We compile the model to configure it for training and use categorical crossentropy as our loss function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvQm0ofZ2q13",
        "colab_type": "code",
        "outputId": "50e63fb1-4d43-45cb-c9bb-d4651c7acf10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "source": [
        "lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "lstm_model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 10, 256)           3797504   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 10, 128)           197120    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 10, 128)           0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 64)                49408     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 14834)             964210    \n",
            "=================================================================\n",
            "Total params: 5,008,242\n",
            "Trainable params: 5,008,242\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orIlNcpMhAW9",
        "colab_type": "text"
      },
      "source": [
        "### Configure Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7E-18Ssn2_wr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE =256\n",
        "file_path = \"/content/PyDataEHV_workshop/TextGeneration/checkpoints/Shakespeare/Shakespeare-LSTM-epoch{epoch:03d}-words%d-sequence%d-batchsize%d-\" \\\n",
        "            \"loss{loss:.4f}-acc{acc:.4f}-val_loss{val_loss:.4f}-val_acc{val_acc:.4f}.hdf5\" % \\\n",
        "            (VOCAB_LEN, SEQ_LEN, BATCH_SIZE)\n",
        "\n",
        "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', save_best_only=True)\n",
        "early_stopping = EarlyStopping(monitor='val_acc', patience=10)\n",
        "callbacks_list = [checkpoint, early_stopping]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9exgAGqnywTd",
        "colab_type": "text"
      },
      "source": [
        "Uncomment if you want do not want to train the model from scratch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8htx6apKyooo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#lstm_model.load_weights('/content/PyDataEHV_workshop/TextGeneration/checkpoints/Shakespeare/Shakespeare-LSTM-epoch011-words14834-sequence10-batchsize256-loss3.6790-acc0.3917-val_loss4.1451-val_acc0.3607.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0ZBBSeghFB8",
        "colab_type": "text"
      },
      "source": [
        "###Train the LSTM Model\n",
        "\n",
        "The model is trained for 30 epochs, but training could stop early due to the callback. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YokKvI_X2zeQ",
        "colab_type": "code",
        "outputId": "f2f828df-b3b5-4a6e-fcb3-0b5246f35544",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774
        }
      },
      "source": [
        "lstm_model.fit_generator(generator(train_input, train_output, BATCH_SIZE),\n",
        "                    steps_per_epoch=int(len(train_input)/BATCH_SIZE) + 1,\n",
        "                    epochs=30,\n",
        "                    callbacks=callbacks_list,\n",
        "                    validation_data=generator(test_input, test_output, BATCH_SIZE),\n",
        "                    validation_steps=int(len(test_input)/BATCH_SIZE) + 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1142/1142 [==============================] - 63s 55ms/step - loss: 5.6401 - acc: 0.1745 - val_loss: 5.5100 - val_acc: 0.1752\n",
            "Epoch 2/30\n",
            "1142/1142 [==============================] - 61s 53ms/step - loss: 5.4903 - acc: 0.1749 - val_loss: 5.5210 - val_acc: 0.1746\n",
            "Epoch 3/30\n",
            "1142/1142 [==============================] - 60s 53ms/step - loss: 5.4848 - acc: 0.1749 - val_loss: 5.5218 - val_acc: 0.1751\n",
            "Epoch 4/30\n",
            "1142/1142 [==============================] - 58s 50ms/step - loss: 5.0334 - acc: 0.2148 - val_loss: 4.6154 - val_acc: 0.3197\n",
            "Epoch 5/30\n",
            "1142/1142 [==============================] - 57s 50ms/step - loss: 4.3901 - acc: 0.3345 - val_loss: 4.3509 - val_acc: 0.3465\n",
            "Epoch 6/30\n",
            "1142/1142 [==============================] - 57s 50ms/step - loss: 4.1865 - acc: 0.3486 - val_loss: 4.2515 - val_acc: 0.3520\n",
            "Epoch 7/30\n",
            "1142/1142 [==============================] - 57s 50ms/step - loss: 4.0441 - acc: 0.3609 - val_loss: 4.1938 - val_acc: 0.3557\n",
            "Epoch 8/30\n",
            "1142/1142 [==============================] - 57s 50ms/step - loss: 3.9306 - acc: 0.3709 - val_loss: 4.1393 - val_acc: 0.3585\n",
            "Epoch 9/30\n",
            "1142/1142 [==============================] - 55s 48ms/step - loss: 3.8368 - acc: 0.3793 - val_loss: 4.1525 - val_acc: 0.3578\n",
            "Epoch 10/30\n",
            "1142/1142 [==============================] - 56s 49ms/step - loss: 3.7529 - acc: 0.3864 - val_loss: 4.1363 - val_acc: 0.3591\n",
            "Epoch 11/30\n",
            "1142/1142 [==============================] - 56s 49ms/step - loss: 3.6790 - acc: 0.3917 - val_loss: 4.1451 - val_acc: 0.3607\n",
            "Epoch 12/30\n",
            "1142/1142 [==============================] - 61s 53ms/step - loss: 3.6085 - acc: 0.3976 - val_loss: 4.1698 - val_acc: 0.3600\n",
            "Epoch 13/30\n",
            "1142/1142 [==============================] - 61s 53ms/step - loss: 3.5401 - acc: 0.4029 - val_loss: 4.1644 - val_acc: 0.3603\n",
            "Epoch 14/30\n",
            "1142/1142 [==============================] - 61s 54ms/step - loss: 3.4793 - acc: 0.4071 - val_loss: 4.1960 - val_acc: 0.3597\n",
            "Epoch 15/30\n",
            "1142/1142 [==============================] - 61s 53ms/step - loss: 3.4210 - acc: 0.4118 - val_loss: 4.2354 - val_acc: 0.3573\n",
            "Epoch 16/30\n",
            "1142/1142 [==============================] - 61s 53ms/step - loss: 3.3665 - acc: 0.4163 - val_loss: 4.2386 - val_acc: 0.3585\n",
            "Epoch 17/30\n",
            "1142/1142 [==============================] - 61s 54ms/step - loss: 3.3153 - acc: 0.4200 - val_loss: 4.2931 - val_acc: 0.3581\n",
            "Epoch 18/30\n",
            "1142/1142 [==============================] - 61s 53ms/step - loss: 3.2654 - acc: 0.4238 - val_loss: 4.3200 - val_acc: 0.3583\n",
            "Epoch 19/30\n",
            "1142/1142 [==============================] - 61s 53ms/step - loss: 3.2199 - acc: 0.4279 - val_loss: 4.3216 - val_acc: 0.3555\n",
            "Epoch 20/30\n",
            "1142/1142 [==============================] - 61s 54ms/step - loss: 3.1782 - acc: 0.4303 - val_loss: 4.3761 - val_acc: 0.3543\n",
            "Epoch 21/30\n",
            "1142/1142 [==============================] - 61s 54ms/step - loss: 3.1350 - acc: 0.4340 - val_loss: 4.3875 - val_acc: 0.3556\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3e478b7898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJfu6wIMhROr",
        "colab_type": "text"
      },
      "source": [
        "###Generate Text "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC83iYZThV7K",
        "colab_type": "text"
      },
      "source": [
        "####Restore the last checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxITpYOrhZa-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_model.load_weights('/content/PyDataEHV_workshop/TextGeneration/checkpoints/Shakespeare/Shakespeare-LSTM-epoch011-words14834-sequence10-batchsize256-loss3.6790-acc0.3917-val_loss4.1451-val_acc0.3607.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7c-E1Kuhc1H",
        "colab_type": "text"
      },
      "source": [
        "####Unconditional Text Generation\n",
        "\n",
        "Starting from a random seed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmByM7NNZgbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generate_uncond_samples(no_of_words = 500, temp=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9dmYBLzhjMn",
        "colab_type": "text"
      },
      "source": [
        "####Conditional Text Generation\n",
        "\n",
        "Enter a starting prompt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04_xTrmiZjF6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generate_cond_samples(no_of_words = 500, temp=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}