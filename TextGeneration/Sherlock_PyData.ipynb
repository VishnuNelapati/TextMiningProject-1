{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sherlock-PyData.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UW2vOf4RCfyV",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dimtr/PyDataEHV_workshop/blob/master/TextGeneration/Sherlock_PyData.ipynb\" target=\"_parent\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIkKrYoVSIcI",
        "colab_type": "text"
      },
      "source": [
        "# Text Generation with LSTM\n",
        "In this workshop, we see how recurrent Neural Networks could be used as Generative Models. They can learn the sequences of a problem and generate entirely new plausible sequences for the problem domain.\n",
        "\n",
        "We will discover how to create a simple text generation model using Python in [Keras](https://keras.io/) that generates text, word-by-word. We will work with the dataset of Sherlock Holmes's stories retrieved from eBook savailable on Project Gutenberg. \n",
        "\n",
        "Given a sequence of words, the model trained on our dataset will predict the next most probable word. We will call the model repeatedly to generate longer sequences.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYTDXoAXXQaH",
        "colab_type": "text"
      },
      "source": [
        "##Setup\n",
        "\n",
        "####Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPl_k7f8qANu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW8XEFpPaG6t",
        "colab_type": "text"
      },
      "source": [
        "####Import Keras and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ENSoJKsaRGM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout,SimpleRNN, LSTM, Embedding\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import LambdaCallback, EarlyStopping\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.utils import shuffle\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Otr5uy-y7rs",
        "colab_type": "text"
      },
      "source": [
        "#### Clone GitHub Repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLXniWMUxmNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/dimtr/PyDataEHV_workshop/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1SCfRbkb2Fw",
        "colab_type": "text"
      },
      "source": [
        "####Reading the data\n",
        "\n",
        "We take a look at the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0s5PSeNDtHkl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files = glob('/content/PyDataEHV_workshop/TextGeneration/datasets/sherlock/*')\n",
        "stories = []\n",
        "for doc in files:\n",
        "  with open(doc, encoding='utf-8') as f:\n",
        "    story = f.readlines()\n",
        "    stories.append(story[4:6]+story[10:-26])\n",
        "\n",
        "print(''.join(stories[0][:20]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0T95x2Z3ftl",
        "colab_type": "text"
      },
      "source": [
        "##Process the Text\n",
        "Before training the model, we need to process the text in a form that is interpretable by the model.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHCR_I-Web-j",
        "colab_type": "text"
      },
      "source": [
        "#### Story in words\n",
        "\n",
        "First, we convert the story data into chunks of words or tokens. Since we want our model to also recognize puntuations as words, we use [replace()](https://docs.python.org/2/library/string.html#string.replace) to add white spaces around them and then use [split()](https://docs.python.org/2/library/stdtypes.html#str.split) to split the story into word chunks. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEz83f1rcm7c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "story_in_words = []\n",
        "for story in stories:\n",
        "  temp = []\n",
        "  for i, line in enumerate(story):\n",
        "    story[i] = line.lower().replace('.', ' . ').\\\n",
        "                            replace('\\n', ' \\n ').\\\n",
        "                            replace(',', ' , ').\\\n",
        "                            replace('?', ' ? ').\\\n",
        "                            replace('\"',' \" ').\\\n",
        "                            replace('!', ' ! ').\\\n",
        "                            replace(':', ' : ').\\\n",
        "                            replace(';', ' ; ').\\\n",
        "                            replace('--', ' ').\\\n",
        "                            replace('-', ' ').\\\n",
        "                            replace(',',' , ')\n",
        "\n",
        "    temp.extend(story[i].split(' '))\n",
        "  temp.append('|endofstory|')\n",
        "\n",
        "\n",
        "  story_in_words.append(list(filter(None, temp))) \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtYVe_amqAOP",
        "colab_type": "text"
      },
      "source": [
        "####Creating sequences\n",
        "\n",
        "The next step is to split the entire text into sequences of a certain length. We specify this length by using the hyper parameter SEQ_LEN. Sequence length is the number of words that the generative model would take as input to predict the next word. \n",
        "\n",
        "We go over the story by, shifting by one word at each step and take SEQ_LEN + 1 words in a sequence at a time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SNEyBCWqwgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEQ_LEN = 50\n",
        "step = 1\n",
        "sentences = []\n",
        "for words in story_in_words:\n",
        "  for i in range(0, len(words) - SEQ_LEN, step):\n",
        "    sentences.append(words[i: i + SEQ_LEN + 1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UnyuPcQqxcc",
        "colab_type": "text"
      },
      "source": [
        "####Tokenizer\n",
        "\n",
        "Now we need to map the strings to numeric representation. We want each unique word to be represented by a unique integer number. We use the [Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) class by Keras for this task. The class then calls a function that fits the tokenizer on the our sequences of text and builds an internal vocabulary. \n",
        "\n",
        "Note: 0 is a reserved index in this class that won't be assigned to any word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKKGz-x0qR3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)  # transforms each text sequence into a sequence of integer, where each integer represents a unique word\n",
        "sequences = np.asarray(sequences)                    # converting a list to a numpy array\n",
        "vocab = tokenizer.word_counts                        # Dict object of vocabulary with frequency count\n",
        "\n",
        "VOCAB_LEN = len(tokenizer.word_counts) + 1\n",
        "total = sequences.shape[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5WC2O7JsC6d",
        "colab_type": "code",
        "outputId": "d0ed2879-99dc-45c8-c964-9e1dfa18239f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "VOCAB_LEN"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16461"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtIpG7lb9Fsp",
        "colab_type": "text"
      },
      "source": [
        "### Pre-trained Glove Embedding\n",
        "\n",
        "The STanford NLP Group developed [GloVe](https://nlp.stanford.edu/projects/glove/), an unsupervised learning algorithm for obtaining vector representations for words. Various pre-train word vectors are available on [here](https://nlp.stanford.edu/projects/glove/). For this workshop, we use the 300 dimension vectors, trained on Wikipedia 2014 and the Fifth Edition of English Gigaword text data, having 6B tokens, 400K vocabulary of uncased words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4-yvBkp9FoT",
        "colab_type": "text"
      },
      "source": [
        "The following two code cells demonstrate how to use the pre-trained GloVe embedding as weights for training our Embedding layer. You can uncomment these code lines and try it at home after downloading the word vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PWlwzLZIvFD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "embedding_vector = {}\n",
        "f = open('../input/embeddings/glove.840B.300d/glove.840B.300d.txt')\n",
        "for line in tqdm(f):\n",
        "    value = line.split(' ')\n",
        "    word = value[0]\n",
        "    coef = np.array(value[1:],dtype = 'float32')\n",
        "    embedding_vector[word] = coef\n",
        "\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o11_aDOlIxwq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "embedding_matrix = np.zeros((VOCAB_LEN,300))\n",
        "for word,i in tqdm(vocab.items()):\n",
        "    embedding_value = embedding_vector.get(word)\n",
        "    if embedding_value is not None:\n",
        "        embedding_matrix[i] = embedding_value\n",
        "\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGo2cLSx_e9e",
        "colab_type": "text"
      },
      "source": [
        "Due to time constraints, we use a pre computed embedding matrix numpy file which is much smaller in size to skip the embedding matrix creation time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YISKqBOlAB7y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix = np.load('/content/PyDataEHV_workshop/TextGeneration/emd_matrix.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plEF3tEv0I3f",
        "colab_type": "text"
      },
      "source": [
        "####Training and Test data split\n",
        "\n",
        "Data is shuffled and split into training and test dataset with 15% of the data in the test split. \n",
        "\n",
        "The first *SEQ_LEN* words of the sequence make the input data and the remaining last word is the target data or the next probable word that the model should output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BiZmqykxBLs",
        "colab_type": "code",
        "outputId": "1a16fa37-f2f8-4d2c-bea4-a6b3f7ff23d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "sequences = shuffle(sequences)\n",
        "\n",
        "train_input = sequences[int(total * 0.15):, :-1]\n",
        "train_output = sequences[int(total * 0.15):, -1]\n",
        "\n",
        "test_input = sequences[:int(total * 0.15), :-1]\n",
        "test_output = sequences[:int(total * 0.15), -1]\n",
        "\n",
        "\n",
        "print(\"Input Training Data Shape:\", train_input.shape)\n",
        "print(\"Target Training Data Shape:\", train_output.shape)\n",
        "\n",
        "print(\"Input Test Data Shape:\", test_input.shape)\n",
        "print(\"Target Test Data Shape:\", test_output.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Training Data Shape: (487513, 50)\n",
            "Target Training Data Shape: (487513,)\n",
            "Input Test Data Shape: (86031, 50)\n",
            "Target Test Data Shape: (86031,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVDVyfD00aqt",
        "colab_type": "text"
      },
      "source": [
        "####Build the RNN Model\n",
        "\n",
        "We use the [Keras Sequential Model](https://keras.io/getting-started/sequential-model-guide/) to define the model. To build our simple RNN text generation model, we use four keras layers:\n",
        "\n",
        "\n",
        "*   [keras.layers.Embedding](https://keras.io/layers/embeddings/)\n",
        "*   [keras.layers.SimpleRNN](https://https://keras.io/layers/recurrent/#simplernn)\n",
        "*   [keras.layers.Dropout](https://keras.io/layers/core/#dropout)\n",
        "*   [keras.layers.Dense](https://keras.io/layers/core/#dense)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PMc52m3Afhg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(VOCAB_LEN, 300, weights=[embedding_matrix], input_length=SEQ_LEN, trainable=False))\n",
        "model.add(LSTM(512, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(VOCAB_LEN, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fci7X5txCaKA",
        "colab_type": "text"
      },
      "source": [
        "We compile the model to configure it for training and use categorical crossentropy as our loss function. [link text](https://)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkNBch7EAi1g",
        "colab_type": "code",
        "outputId": "37f151a1-d4a4-4841-e9c3-62b35bee5b6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 50, 256)           4214016   \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, 50, 512)           1574912   \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 50, 512)           0         \n",
            "_________________________________________________________________\n",
            "lstm_8 (LSTM)                (None, 50, 256)           787456    \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 50, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_9 (LSTM)                (None, 128)               197120    \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 16461)             2123469   \n",
            "=================================================================\n",
            "Total params: 8,896,973\n",
            "Trainable params: 8,896,973\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TELUD_KOM9XF",
        "colab_type": "code",
        "outputId": "a4da2dcf-60e8-49aa-997f-c316fabe2937",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 856
        }
      },
      "source": [
        "from keras.utils import plot_model\n",
        "plot_model(model, show_layer_names=False, to_file='/content/PyDataEHV_workshop/TextGeneration/model.png')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAANHCAYAAAAWlyI0AAAABmJLR0QA/wD/AP+gvaeTAAAgAElE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\nF0r/P7EU6JXeZRg9AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2c9sr7U0p1w",
        "colab_type": "text"
      },
      "source": [
        "###Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hOylH2UDxXR",
        "colab_type": "text"
      },
      "source": [
        "We create a generator function that generates data batch-by-batch. The generator is run in parallel to the model, for efficiency. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAMVl9Bexap2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator(sent, word, batch_size):\n",
        "\n",
        "  global index\n",
        "  index = 0\n",
        "  while True:\n",
        "    x = np.zeros((batch_size, SEQ_LEN), dtype=np.int)\n",
        "    y = np.zeros((batch_size, VOCAB_LEN), dtype=np.bool)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "      x[i] = sent[index % len(sent)]\n",
        "      y[i] = to_categorical(word[index % len(word)], num_classes=VOCAB_LEN) #convert integers to one-hot encoded vectors\n",
        "      index = index + 1\n",
        "    yield x,y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0vz5ARoEaxT",
        "colab_type": "text"
      },
      "source": [
        "This function samples an index from a softmax probablity array based on the temperature. This technique is called temperature sampling and is used to improve the quality of samples from language models.\n",
        "\n",
        "Note: The high temperature sample displays greater linguistic variety, but the low temperature sample is more grammatically correct. Lowering the temperature allows you to focus on higher probability output sequences and smooth over deficiencies of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoYihS3CAqru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.flip(np.argsort(probas))[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBLJ2ljjGbp-",
        "colab_type": "text"
      },
      "source": [
        "We make a simple display function that prints the stories "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEgcYSEjKqV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display(story):\n",
        "  full = ''\n",
        "  for word in story:\n",
        "    if word == '|newline|':\n",
        "      full = full + '\\n'\n",
        "    elif word in ',.;:?!':\n",
        "      full = full + word\n",
        "    else:\n",
        "      full = full + ' ' + word\n",
        "  \n",
        "  print(full)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIcVgS7U0u1p",
        "colab_type": "text"
      },
      "source": [
        "####Configure Checkpoints\n",
        "\n",
        "We use two types of checkpoints for our model:\n",
        "\n",
        "\n",
        "*   [ModelCheckpoint](https://keras.io/callbacks/#modelcheckpoint): to ensure that checkpoints are saved during training by monitoring a quality (validation accuracy in this case)\n",
        "*   [EarlyStopping](https://keras.io/callbacks/#EarlyStopping): stops training when the monitored quality (validation accuracy) has not improved for a certain number of epochs. This threshold is set by the *patience* argument.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qCZ5JGuAwCx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE =32\n",
        "file_path = \"/content/PyDataEHV_workshop/TextGeneration/checkpoints/Sherlock/Sherlock-RNN-epoch{epoch:03d}-words%d-sequence%d-batchsize%d-\" \\\n",
        "            \"loss{loss:.4f}-acc{acc:.4f}-val_loss{val_loss:.4f}-val_acc{val_acc:.4f}.hdf5\" % \\\n",
        "            (VOCAB_LEN, SEQ_LEN, BATCH_SIZE)\n",
        "\n",
        "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', save_best_only=True) # latest best model according to the val_acc monitored will not be overwritten\n",
        "early_stopping = EarlyStopping(monitor='val_acc', patience=5)\n",
        "callbacks_list = [checkpoint, early_stopping]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HI-l35Njy_xo"
      },
      "source": [
        "Uncomment if you do not want to train the model from scratch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F78uM5XPy_xp",
        "colab": {}
      },
      "source": [
        "#model.load_weights('/content/PyDataEHV_workshop/TextGeneration/checkpoints/Sherlock/sherlock-RNN-epoch014-words16461-sequence20-batchsize512-loss4.0425-acc0.2680-val_loss4.2978-val_acc0.2620.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tADs8aV8025R",
        "colab_type": "text"
      },
      "source": [
        "####Train the Model\n",
        "\n",
        "The model is trained for 30 epochs, but training could stop early due to the callback. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9m5tcGD2A01J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit_generator(generator(train_input, train_output, BATCH_SIZE),\n",
        "                    steps_per_epoch=int(len(train_input)/BATCH_SIZE) + 1,\n",
        "                    epochs=30,\n",
        "                    callbacks=callbacks_list,\n",
        "                    validation_data=generator(test_input, test_output, BATCH_SIZE),\n",
        "                    validation_steps=int(len(test_input)/BATCH_SIZE) + 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N69K3rsr08qE",
        "colab_type": "text"
      },
      "source": [
        "###Generate Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIiRYkS-1XiM",
        "colab_type": "text"
      },
      "source": [
        "####Restore the latest checkpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vy6b74c1Ihz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model.load_weights('/content/PyDataEHV_workshop/TextGeneration/checkpoints/Sherlock/sherlock-RNN-epoch014-words16461-sequence20-batchsize512-loss4.0425-acc0.2680-val_loss4.2978-val_acc0.2620.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qCTRit71AOg",
        "colab_type": "text"
      },
      "source": [
        "####Conditional Samples\n",
        "\n",
        "Here, the story is generated word-by-word, based on a prompt provided. If the number of words in the prompt exceed SEQ_LEN, then the prompt is truncated from the beginning to fit the sequence length. If the length of prompt is less than SEQ_LEN, then zeros are padded in the beginning. The whole story is genrated by predicting the next probable word in a loop.\n",
        "\n",
        "Note: Since 0 is used to pad sequences, it is important that the Tokenizer does not use 0 as an index. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Lc3_y34AtyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_cond_samples(no_of_words, temp):\n",
        "    \n",
        "    print('Enter prompt: \\n')\n",
        "    seed = input()\n",
        "    sentence = tokenizer.texts_to_sequences([seed])[0]\n",
        "\n",
        "    sentence = list(pad_sequences([sentence], maxlen=SEQ_LEN, padding='pre', truncating='pre')[0])\n",
        "\n",
        "\n",
        "    gen_story = []\n",
        "    gen_story.extend(seed.split())\n",
        "\n",
        "    for i in range(no_of_words):\n",
        "        x_pred = np.expand_dims(sentence, axis=0) \n",
        "            \n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_indices = sample(preds, temp)\n",
        "\n",
        "        for ix in next_indices:\n",
        "          if ix == 0:\n",
        "            continue\n",
        "          else:\n",
        "            next_word = tokenizer.index_word[ix]\n",
        "            sentence = sentence[1:]\n",
        "            sentence.append(ix)\n",
        "            break\n",
        "\n",
        "        gen_story.append(next_word)\n",
        "        \n",
        "    display(gen_story)\n",
        "        \n",
        "       \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISZfU2osSEvJ",
        "colab_type": "code",
        "outputId": "06f52cb0-5c71-4412-cfeb-9b9b5781138e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "source": [
        "generate_cond_samples(no_of_words = 500, temp=0.5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter prompt: \n",
            "\n",
            "Sherlock woke up and saw\n",
            " Sherlock woke up and saw him, and then and that he had been seen and at the last time, and the other was a tall, thin, thin, thin, eager face of the house. he had been there, \" said holmes. \" i have never been a very good man, and his eyes were brought by a very different man, and that he had been taken at the way of a paper, and i am prepared to leave him to be a clue. \" \n",
            " \n",
            " \" yes, i have not no doubt that i should be able to tell me that you have no doubt that it is not a good deal for a clue. \" \n",
            " \n",
            " \" i am sorry, that he had been a perfect knife, and i am sure that i could have been so. he has been seen. i have no doubt that it is a very certain that a very strange man who had been a good deal. \" \n",
            " \n",
            " \" that is a very few minutes of a world, but it was clear that you should be able to learn that you have come out of my own story. \" \n",
            " \n",
            " \" i should not be able to see it. but the man was a fine man. i had a little than i had been a very considerable nature. \" \n",
            " \n",
            " \" i think that i should go to the case of the matter. \" \n",
            " \n",
            " \" i am afraid that i should have been a very quiet man. \" \n",
            " \n",
            " \" i\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjTYRruE1C6k",
        "colab_type": "text"
      },
      "source": [
        "####Unconditional Samples\n",
        "\n",
        "Here, the story is generated word-by-word by starting with a random seed. One integer is randomly sampled from the index of words and story is generated by prediciting the next probable work in a loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFs9KdtS0Zjs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_uncond_samples(no_of_words, temp):\n",
        "    np.random.seed(0)\n",
        "    seed = np.random.randint(1, VOCAB_LEN, size=SEQ_LEN)\n",
        "\n",
        "    sentence = seed\n",
        "\n",
        "    sentence = list(pad_sequences([sentence], maxlen=SEQ_LEN, padding='pre')[0])\n",
        "\n",
        "    gen_story = []\n",
        "\n",
        "    gen_story.extend(tokenizer.index_word[w] for w in seed)\n",
        "    end_flag = 0\n",
        "\n",
        "    for i in range(no_of_words):\n",
        "        x_pred = np.expand_dims(sentence, axis=0)\n",
        "\n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_indices = sample(preds, temp)\n",
        "\n",
        "        for ix in next_indices:\n",
        "            if ix == 0:\n",
        "                continue\n",
        "\n",
        "            elif '|endofstory|' in tokenizer.word_index.keys():\n",
        "                if ix == tokenizer.word_index['|endofstory|'] :\n",
        "                    end_flag = 1\n",
        "                    break\n",
        "            else:\n",
        "                next_word = tokenizer.index_word[ix]\n",
        "                sentence = sentence[1:]\n",
        "                sentence.append(ix)\n",
        "                break\n",
        "\n",
        "        if end_flag == 1:\n",
        "            break\n",
        "\n",
        "\n",
        "        gen_story.append(next_word)\n",
        "\n",
        "    display(gen_story)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRZkP_XDPR2c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generate_uncond_samples(no_of_words = 500, temp=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiFZx-rBTEPR",
        "colab_type": "text"
      },
      "source": [
        "Looking at the generated text, you'll see the model knows when to use punctuations, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCEcBb9hZOPj",
        "colab_type": "text"
      },
      "source": [
        "###Build an LSTM Model\n",
        "\n",
        "We keep the structure for the model similar but use the Keras LSTM layer instead of the SimpleRNN layer. We again use the [Keras Sequential Model](https://keras.io/getting-started/sequential-model-guide/) to define the LSTM based generative model by using four keras layers:\n",
        "\n",
        "\n",
        "*   [keras.layers.Embedding](https://keras.io/layers/embeddings/): used to train a dense representation of words and their relative meanings.\n",
        "*   [keras.layers.LSTM](https://keras.io/layers/recurrent/#lstm): long-short term memory layer composed of a *cell*, an *input gate*, an *output gate* and a *forget gate*.\n",
        "*   [keras.layers.Dropout](https://keras.io/layers/core/#dropout): applies a regularization technique where randomly selected neurons are ignored or \"dropped-out\" during training.\n",
        "*   [keras.layers.Dense](https://keras.io/layers/core/#dense): regular densely connected neural network layer with output size equal to the vocabulary size (number of unique words). This layer is added at the end and uses the softmax activation to output the probablities (that add up to one) for each word. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9P1k4nAyi7Z-",
        "colab": {}
      },
      "source": [
        "lstm_model = Sequential()\n",
        "lstm_model.add(Embedding(VOCAB_LEN, 256, input_length=SEQ_LEN))\n",
        "lstm_model.add(LSTM(128, return_sequences=True))\n",
        "lstm_model.add(Dropout(0.2))\n",
        "lstm_model.add(LSTM(64))\n",
        "lstm_model.add(Dropout(0.2))\n",
        "lstm_model.add(Dense(VOCAB_LEN, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m70vEiAEg5tc",
        "colab_type": "text"
      },
      "source": [
        "We compile the model to configure it for training and use categorical crossentropy as our loss function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "63e06ebb-d010-43b9-98b2-7ba9bb2b5663",
        "id": "RXcR-PMSi7aB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "source": [
        "lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "lstm_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 10, 256)           5127936   \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 10, 128)           197120    \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 10, 128)           0         \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 64)                49408     \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 20031)             1302015   \n",
            "=================================================================\n",
            "Total params: 6,676,479\n",
            "Trainable params: 6,676,479\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orIlNcpMhAW9",
        "colab_type": "text"
      },
      "source": [
        "### Configure Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B6eOlrO5i7aF",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE =256\n",
        "file_path = \"/content/PyDataEHV_workshop/TextGeneration/checkpoints/Sherlock/Sherlock-LSTM-epoch{epoch:03d}-words%d-sequence%d-batchsize%d-\" \\\n",
        "            \"loss{loss:.4f}-acc{acc:.4f}-val_loss{val_loss:.4f}-val_acc{val_acc:.4f}.hdf5\" % \\\n",
        "            (VOCAB_LEN, SEQ_LEN, BATCH_SIZE)\n",
        "\n",
        "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', save_best_only=True)\n",
        "early_stopping = EarlyStopping(monitor='val_acc', patience=10)\n",
        "callbacks_list = [checkpoint, early_stopping]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlEajkmjxwv9",
        "colab_type": "text"
      },
      "source": [
        "Uncomment if you do not want to train the model from scratch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1tgi7MZoGn7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model.load_weights('/content/PyDataEHV_workshop/TextGeneration/checkpoints/Sherlock/sherlock-LSTM-epoch021-words16461-sequence20-batchsize512-loss3.4346-acc0.3279-val_loss4.4792-val_acc0.2852.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0ZBBSeghFB8",
        "colab_type": "text"
      },
      "source": [
        "###Train the LSTM Model\n",
        "\n",
        "The model is trained for 30 epochs, but training could stop early due to the callback. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0f80b6c1-d9d1-43ce-c6d2-1bef2aaf893a",
        "id": "El0XmBv5i7aH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "lstm_model.fit_generator(generator(train_input, train_output, BATCH_SIZE),\n",
        "                    steps_per_epoch=int(len(train_input)/BATCH_SIZE) + 1,\n",
        "                    epochs=30,\n",
        "                    callbacks=callbacks_list,\n",
        "                    validation_data=generator(test_input, test_output, BATCH_SIZE),\n",
        "                    validation_steps=int(len(test_input)/BATCH_SIZE) + 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "2967/2967 [==============================] - 193s 65ms/step - loss: 4.7060 - acc: 0.3779 - val_loss: 4.6633 - val_acc: 0.3739\n",
            "Epoch 2/100\n",
            "2967/2967 [==============================] - 193s 65ms/step - loss: 4.6286 - acc: 0.3780 - val_loss: 4.6606 - val_acc: 0.3736\n",
            "Epoch 3/100\n",
            "2967/2967 [==============================] - 192s 65ms/step - loss: 4.1183 - acc: 0.4032 - val_loss: 3.6000 - val_acc: 0.4620\n",
            "Epoch 4/100\n",
            "2967/2967 [==============================] - 191s 64ms/step - loss: 3.4038 - acc: 0.4760 - val_loss: 3.3236 - val_acc: 0.4872\n",
            "Epoch 5/100\n",
            "2967/2967 [==============================] - 191s 64ms/step - loss: 3.1931 - acc: 0.4954 - val_loss: 3.2089 - val_acc: 0.4966\n",
            "Epoch 6/100\n",
            "2967/2967 [==============================] - 191s 64ms/step - loss: 3.0667 - acc: 0.5052 - val_loss: 3.1520 - val_acc: 0.5016\n",
            "Epoch 7/100\n",
            "2967/2967 [==============================] - 192s 65ms/step - loss: 2.9717 - acc: 0.5126 - val_loss: 3.1142 - val_acc: 0.5065\n",
            "Epoch 8/100\n",
            "2967/2967 [==============================] - 195s 66ms/step - loss: 2.8984 - acc: 0.5179 - val_loss: 3.0980 - val_acc: 0.5084\n",
            "Epoch 9/100\n",
            "2967/2967 [==============================] - 190s 64ms/step - loss: 2.8413 - acc: 0.5220 - val_loss: 3.0879 - val_acc: 0.5101\n",
            "Epoch 10/100\n",
            "2967/2967 [==============================] - 190s 64ms/step - loss: 2.7914 - acc: 0.5262 - val_loss: 3.0779 - val_acc: 0.5121\n",
            "Epoch 11/100\n",
            "2967/2967 [==============================] - 190s 64ms/step - loss: 2.7479 - acc: 0.5293 - val_loss: 3.0772 - val_acc: 0.5121\n",
            "Epoch 12/100\n",
            "2967/2967 [==============================] - 191s 64ms/step - loss: 2.7096 - acc: 0.5324 - val_loss: 3.0833 - val_acc: 0.5120\n",
            "Epoch 13/100\n",
            "2967/2967 [==============================] - 191s 64ms/step - loss: 2.6753 - acc: 0.5353 - val_loss: 3.0843 - val_acc: 0.5144\n",
            "Epoch 14/100\n",
            "2967/2967 [==============================] - 191s 64ms/step - loss: 2.6432 - acc: 0.5378 - val_loss: 3.0945 - val_acc: 0.5134\n",
            "Epoch 15/100\n",
            "2967/2967 [==============================] - 190s 64ms/step - loss: 2.6166 - acc: 0.5401 - val_loss: 3.1055 - val_acc: 0.5117\n",
            "Epoch 16/100\n",
            "2967/2967 [==============================] - 190s 64ms/step - loss: 2.5880 - acc: 0.5430 - val_loss: 3.1112 - val_acc: 0.5134\n",
            "Epoch 17/100\n",
            "2967/2967 [==============================] - 194s 65ms/step - loss: 2.5626 - acc: 0.5450 - val_loss: 3.1292 - val_acc: 0.5134\n",
            "Epoch 18/100\n",
            "2967/2967 [==============================] - 191s 64ms/step - loss: 2.5376 - acc: 0.5475 - val_loss: 3.1352 - val_acc: 0.5124\n",
            "Epoch 19/100\n",
            "2967/2967 [==============================] - 189s 64ms/step - loss: 2.5159 - acc: 0.5488 - val_loss: 3.1421 - val_acc: 0.5134\n",
            "Epoch 20/100\n",
            "2657/2967 [=========================>....] - ETA: 18s - loss: 2.4916 - acc: 0.5518Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJfu6wIMhROr",
        "colab_type": "text"
      },
      "source": [
        "###Generate Text "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC83iYZThV7K",
        "colab_type": "text"
      },
      "source": [
        "####Restore the last checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxITpYOrhZa-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights('/content/PyDataEHV_workshop/TextGeneration/checkpoints/Sherlock/sherlock-LSTM-epoch021-words16461-sequence20-batchsize512-loss3.4346-acc0.3279-val_loss4.4792-val_acc0.2852.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7c-E1Kuhc1H",
        "colab_type": "text"
      },
      "source": [
        "####Unconditional Text Generation\n",
        "\n",
        "Starting from a random seed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmByM7NNZgbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generate_uncond_samples(no_of_words = 500, temp=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9dmYBLzhjMn",
        "colab_type": "text"
      },
      "source": [
        "####Conditional Text Generation\n",
        "\n",
        "Enter a starting prompt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04_xTrmiZjF6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generate_cond_samples(no_of_words = 500, temp=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}