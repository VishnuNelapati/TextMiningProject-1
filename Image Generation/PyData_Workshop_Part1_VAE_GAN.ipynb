{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/dimtr/PyDataEHV_workshop/blob/master/Image%20Generation/PyData_Workshop_Part1_VAE_GAN.ipynb\" target=\"_parent\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "\n",
    "\n",
    "## VAE-GAN ([article](https://arxiv.org/abs/1512.09300)) \n",
    "<img src=\"Images/VAE_GAN.PNG\" width=\"400\">\n",
    "\n",
    "VAE-GAN combines the VAE and GAN to autoencode over a latent representation of data in the generator to improve over the pixelwise error function used in autoencoders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### install necessary packages if in colab\n",
    "def run_subprocess_command(cmd):\n",
    "    process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
    "    for line in process.stdout:\n",
    "        print(line.decode().strip())\n",
    "\n",
    "\n",
    "import sys, subprocess\n",
    "\n",
    "colab_requirements = [\n",
    "    \"pip install tf-nightly-gpu-2.0-preview==2.0.0.dev20190513\",\n",
    "    \"pip install tfp-nightly==0.7.0.dev20190508\",\n",
    "]\n",
    "\n",
    "\n",
    "for i in colab_requirements:\n",
    "        run_subprocess_command(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.autonotebook import tqdm\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "import pandas as pd\n",
    "import tensorflow_probability as tfp\n",
    "ds = tfp.distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a fashion-MNIST dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BUF=60000\n",
    "BATCH_SIZE=64\n",
    "TEST_BUF=10000\n",
    "DIMS = (28,28,1)\n",
    "N_TRAIN_BATCHES =int(TRAIN_BUF/BATCH_SIZE)\n",
    "N_TEST_BATCHES = int(TEST_BUF/BATCH_SIZE)\n",
    "\n",
    "# load dataset\n",
    "(train_images, _), (test_images, _) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# split dataset\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype(\n",
    "    \"float32\"\n",
    ") / 255.0\n",
    "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1).astype(\"float32\") / 255.0\n",
    "\n",
    "# batch datasets\n",
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(train_images)\n",
    "    .shuffle(TRAIN_BUF)\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "test_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(test_images)\n",
    "    .shuffle(TEST_BUF)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the VAE-GAN network using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEGAN(tf.keras.Model):\n",
    "    \"\"\"a VAEGAN class for tensorflow\n",
    "    \n",
    "    Extends:\n",
    "        tf.keras.Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(VAEGAN, self).__init__()\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        self.enc = tf.keras.Sequential(self.enc)\n",
    "        self.dec = tf.keras.Sequential(self.dec)\n",
    "        inputs, disc_l, outputs = self.vae_disc_function()\n",
    "        self.disc = tf.keras.Model(inputs=[inputs], outputs=[outputs, disc_l])\n",
    "\n",
    "        self.enc_optimizer = tf.keras.optimizers.Adam(self.lr_base_gen, beta_1=0.5)\n",
    "        self.dec_optimizer = tf.keras.optimizers.Adam(self.lr_base_gen, beta_1=0.5)\n",
    "        self.disc_optimizer = tf.keras.optimizers.Adam(self.get_lr_d, beta_1=0.5)\n",
    "\n",
    "    def encode(self, x):\n",
    "        mu, sigma = tf.split(self.enc(x), num_or_size_splits=2, axis=1)\n",
    "        return mu, sigma\n",
    "\n",
    "    def dist_encode(self, x):\n",
    "        mu, sigma = self.encode(x)\n",
    "        return ds.MultivariateNormalDiag(loc=mu, scale_diag=sigma)\n",
    "\n",
    "    def get_lr_d(self):\n",
    "        return self.lr_base_disc * self.D_prop\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.dec(z)\n",
    "\n",
    "    def discriminate(self, x):\n",
    "        return self.disc(x)\n",
    "\n",
    "    def reconstruct(self, x):\n",
    "        mean, _ = self.encode(x)\n",
    "        return self.decode(mean)\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * 0.5) + mean\n",
    "\n",
    "    # @tf.function\n",
    "    def compute_loss(self, x):\n",
    "        # pass through network\n",
    "        q_z = self.dist_encode(x)\n",
    "        z = q_z.sample()\n",
    "        p_z = ds.MultivariateNormalDiag(\n",
    "            loc=[0.0] * z.shape[-1], scale_diag=[1.0] * z.shape[-1]\n",
    "        )\n",
    "        xg = self.decode(z)\n",
    "        z_samp = tf.random.normal([x.shape[0], 1, 1, z.shape[-1]])\n",
    "        xg_samp = self.decode(z_samp)\n",
    "        d_xg, ld_xg = self.discriminate(xg)\n",
    "        d_x, ld_x = self.discriminate(x)\n",
    "        d_xg_samp, ld_xg_samp = self.discriminate(xg_samp)\n",
    "\n",
    "        # GAN losses\n",
    "        disc_real_loss = gan_loss(logits=d_x, is_real=True)\n",
    "        disc_fake_loss = gan_loss(logits=d_xg_samp, is_real=False)\n",
    "        gen_fake_loss = gan_loss(logits=d_xg_samp, is_real=True)\n",
    "\n",
    "        discrim_layer_recon_loss = (\n",
    "            tf.reduce_mean(tf.reduce_mean(tf.math.square(ld_x - ld_xg), axis=0))\n",
    "            / self.recon_loss_div\n",
    "        )\n",
    "\n",
    "        self.D_prop = sigmoid(\n",
    "            disc_fake_loss - gen_fake_loss, shift=0.0, mult=self.sig_mult\n",
    "        )\n",
    "\n",
    "        kl_div = ds.kl_divergence(q_z, p_z)\n",
    "        latent_loss = tf.reduce_mean(tf.maximum(kl_div, 0)) / self.latent_loss_div\n",
    "\n",
    "        return (\n",
    "            self.D_prop,\n",
    "            latent_loss,\n",
    "            discrim_layer_recon_loss,\n",
    "            gen_fake_loss,\n",
    "            disc_fake_loss,\n",
    "            disc_real_loss,\n",
    "        )\n",
    "\n",
    "    # @tf.function\n",
    "    def compute_gradients(self, x):\n",
    "        with tf.GradientTape() as enc_tape, tf.GradientTape() as dec_tape, tf.GradientTape() as disc_tape:\n",
    "            (\n",
    "                _,\n",
    "                latent_loss,\n",
    "                discrim_layer_recon_loss,\n",
    "                gen_fake_loss,\n",
    "                disc_fake_loss,\n",
    "                disc_real_loss,\n",
    "            ) = self.compute_loss(x)\n",
    "\n",
    "            enc_loss = latent_loss + discrim_layer_recon_loss\n",
    "            dec_loss = gen_fake_loss + discrim_layer_recon_loss\n",
    "            disc_loss = disc_fake_loss + disc_real_loss\n",
    "\n",
    "        enc_gradients = enc_tape.gradient(enc_loss, self.enc.trainable_variables)\n",
    "        dec_gradients = dec_tape.gradient(dec_loss, self.dec.trainable_variables)\n",
    "        disc_gradients = disc_tape.gradient(disc_loss, self.disc.trainable_variables)\n",
    "\n",
    "        return enc_gradients, dec_gradients, disc_gradients\n",
    "\n",
    "    @tf.function\n",
    "    def apply_gradients(self, enc_gradients, dec_gradients, disc_gradients):\n",
    "        self.enc_optimizer.apply_gradients(\n",
    "            zip(enc_gradients, self.enc.trainable_variables)\n",
    "        )\n",
    "        self.dec_optimizer.apply_gradients(\n",
    "            zip(dec_gradients, self.dec.trainable_variables)\n",
    "        )\n",
    "        self.disc_optimizer.apply_gradients(\n",
    "            zip(disc_gradients, self.disc.trainable_variables)\n",
    "        )\n",
    "\n",
    "    def train(self, x):\n",
    "        enc_gradients, dec_gradients, disc_gradients = self.compute_gradients(x)\n",
    "        self.apply_gradients(enc_gradients, dec_gradients, disc_gradients)\n",
    "\n",
    "\n",
    "def gan_loss(logits, is_real=True):\n",
    "    \"\"\"Computes standard gan loss between logits and labels\n",
    "                \n",
    "        Arguments:\n",
    "            logits {[type]} -- output of discriminator\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            isreal {bool} -- whether labels should be 0 (fake) or 1 (real) (default: {True})\n",
    "        \"\"\"\n",
    "    if is_real:\n",
    "        labels = tf.ones_like(logits)\n",
    "    else:\n",
    "        labels = tf.zeros_like(logits)\n",
    "\n",
    "    return tf.compat.v1.losses.sigmoid_cross_entropy(\n",
    "        multi_class_labels=labels, logits=logits\n",
    "    )\n",
    "\n",
    "\n",
    "def sigmoid(x, shift=0.0, mult=20):\n",
    "    \"\"\" squashes a value with a sigmoid\n",
    "    \"\"\"\n",
    "    return tf.constant(1.0) / (\n",
    "        tf.constant(1.0) + tf.exp(-tf.constant(1.0) * (x * mult))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network architecture\n",
    "- the network has an variational autoencoder as its generator, and a UNET as its descriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_Z =128\n",
    "encoder = [\n",
    "    tf.keras.layers.InputLayer(input_shape=DIMS),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=32, kernel_size=3, strides=(2, 2), activation=\"relu\"\n",
    "    ),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=64, kernel_size=3, strides=(2, 2), activation=\"relu\"\n",
    "    ),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=N_Z*2),\n",
    "]\n",
    "\n",
    "decoder = [\n",
    "    tf.keras.layers.Dense(units=7 * 7 * 64, activation=\"relu\"),\n",
    "    tf.keras.layers.Reshape(target_shape=(7, 7, 64)),\n",
    "    tf.keras.layers.Conv2DTranspose(\n",
    "        filters=64, kernel_size=3, strides=(2, 2), padding=\"SAME\", activation=\"relu\"\n",
    "    ),\n",
    "    tf.keras.layers.Conv2DTranspose(\n",
    "        filters=32, kernel_size=3, strides=(2, 2), padding=\"SAME\", activation=\"relu\"\n",
    "    ),\n",
    "    tf.keras.layers.Conv2DTranspose(\n",
    "        filters=1, kernel_size=3, strides=(1, 1), padding=\"SAME\", activation=\"sigmoid\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "def vaegan_discrim():\n",
    "    inputs = tf.keras.layers.Input(shape=(28, 28, 1))\n",
    "    conv1 = tf.keras.layers.Conv2D(\n",
    "                filters=32, kernel_size=3, strides=(2, 2), activation=\"relu\"\n",
    "            )(inputs)\n",
    "    conv2 = tf.keras.layers.Conv2D(\n",
    "                filters=64, kernel_size=3, strides=(2, 2), activation=\"relu\"\n",
    "            )(conv1)\n",
    "    flatten = tf.keras.layers.Flatten()(conv2)\n",
    "    lastlayer = tf.keras.layers.Dense(units=512, activation=\"relu\")(flatten)\n",
    "    outputs = tf.keras.layers.Dense(units=1, activation = None)(lastlayer)\n",
    "    return inputs, lastlayer, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the unet function \n",
    "gen_optimizer = tf.keras.optimizers.Adam(1e-3, beta_1=0.5)\n",
    "disc_optimizer = tf.keras.optimizers.RMSprop(1e-3)\n",
    "    \n",
    "# model\n",
    "model = VAEGAN(\n",
    "    enc = encoder,\n",
    "    dec = decoder,\n",
    "    vae_disc_function = vaegan_discrim,\n",
    "    lr_base_gen = 1e-3, # \n",
    "    lr_base_disc = 1e-4, # the discriminator's job is easier than the generators so make the learning rate lower\n",
    "    latent_loss_div=1, # this variable will depend on your dataset - choose a number that will bring your latent loss to ~1-10\n",
    "    sig_mult = 10, # how binary the discriminator's learning rate is shifted (we squash it with a sigmoid)\n",
    "    recon_loss_div = .001, # this variable will depend on your dataset - choose a number that will bring your latent loss to ~1-10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exampled data for plotting results\n",
    "example_data = next(iter(train_dataset))\n",
    "model.train(example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reconstruction(model, example_data, nex=8, zm=2):\n",
    "\n",
    "    example_data_reconstructed = model.reconstruct(example_data)\n",
    "    samples = model.decode(tf.random.normal(shape=(BATCH_SIZE, N_Z)))\n",
    "    fig, axs = plt.subplots(ncols=nex, nrows=3, figsize=(zm * nex, zm * 3))\n",
    "    for axi, (dat, lab) in enumerate(\n",
    "        zip(\n",
    "            [example_data, example_data_reconstructed, samples],\n",
    "            [\"data\", \"data recon\", \"samples\"],\n",
    "        )\n",
    "    ):\n",
    "        for ex in range(nex):\n",
    "            axs[axi, ex].matshow(\n",
    "                dat.numpy()[ex].squeeze(), cmap=plt.cm.Greys, vmin=0, vmax=1\n",
    "            )\n",
    "            axs[axi, ex].axes.get_xaxis().set_ticks([])\n",
    "            axs[axi, ex].axes.get_yaxis().set_ticks([])\n",
    "        axs[axi, 0].set_ylabel(lab)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def plot_losses(losses):\n",
    "    fig, axs =plt.subplots(ncols = 4, nrows = 1, figsize= (16,4))\n",
    "    axs[0].plot(losses.latent_loss.values, label = 'latent_loss')\n",
    "    axs[1].plot(losses.discrim_layer_recon_loss.values, label = 'discrim_layer_recon_loss')\n",
    "    axs[2].plot(losses.disc_real_loss.values, label = 'disc_real_loss')\n",
    "    axs[2].plot(losses.disc_fake_loss.values, label = 'disc_fake_loss')\n",
    "    axs[2].plot(losses.gen_fake_loss.values, label = 'gen_fake_loss')\n",
    "    axs[3].plot(losses.d_prop.values, label = 'd_prop')\n",
    "\n",
    "    for ax in axs.flatten():\n",
    "        ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a pandas dataframe to save the loss information to\n",
    "losses = pd.DataFrame(columns=[\n",
    "    'd_prop',\n",
    "    'latent_loss',\n",
    "    'discrim_layer_recon_loss',\n",
    "    'gen_fake_loss',\n",
    "    'disc_fake_loss',\n",
    "    'disc_real_loss',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    # train\n",
    "    for batch, train_x in tqdm(\n",
    "        zip(range(N_TRAIN_BATCHES), train_dataset), total=N_TRAIN_BATCHES\n",
    "    ):\n",
    "        model.train(train_x)\n",
    "    # test on holdout\n",
    "    loss = []\n",
    "    for batch, test_x in tqdm(\n",
    "        zip(range(N_TEST_BATCHES), train_dataset), total=N_TEST_BATCHES\n",
    "    ):\n",
    "        loss.append(model.compute_loss(train_x))\n",
    "    losses.loc[len(losses)] = np.mean(loss, axis=0)\n",
    "    # plot results\n",
    "    display.clear_output()\n",
    "    print(\n",
    "        \"Epoch: {}\".format(epoch)\n",
    "    )\n",
    "    plot_reconstruction(model, example_data)\n",
    "    plot_losses(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generative",
   "language": "python",
   "name": "generative"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
