# PyData Eindhoven

Python notebooks on deep generative models for image and text generations for PyData Eindhoven November 2019.

The [Image Generation](https://github.com/dimtr/PyDataEHV_workshop/tree/master/Image%20Generation) directory contains the notebooks for image generation using VAE and GAN trained on Fashion MNIST dataset by ZalandoResearch.

The [Text Generation](https://github.com/dimtr/PyDataEHV_workshop/tree/master/TextGeneration) directory contains the notebooks for text generation based on RNN and LSTM trained on Shakespeare and Sherlock Holmes text dataset.
The text data can be found in the [datasets](https://github.com/dimtr/PyDataEHV_workshop/tree/master/TextGeneration/datasets) directory. Several checkpoints are included to skip the training step. Load the appropriate checkpoint and generate conditional or unconditional texts in Shakespeare style.


### References
* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
* [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
* [Illustrated Guide to LSTM’s and GRU’s: A step by step explanation](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)


### Further Reading
* [Attention Is All You Need](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)
* [Attention Mechanism](https://blog.floydhub.com/attention-mechanism/)
* [Attention? Attention!](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)
* [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
* [The Illustrated GPT-2 (Visualizing Transformer Language Models)](http://jalammar.github.io/illustrated-gpt2/)