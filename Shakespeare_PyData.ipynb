{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Shakespeare PyData.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hofsF4rCsd8k",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dimtr/PyDataEHV_workshop/blob/master/Shakespeare_PyData.ipynb\" target=\"_parent\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIkKrYoVSIcI",
        "colab_type": "text"
      },
      "source": [
        "# Text Generation with Recurrent Neural Networks\n",
        "In this workshop, we see how recurrent Neural Networks could be used as Generative Models. They can learn the sequences of a problem and generate entirely new plausible sequences for the problem domain.\n",
        "\n",
        "We will discover how to create a simple text generation model using Python in [Keras](https://keras.io/) that generates text, word-by-word. We will work with the dataset of Shakespeare's writing (from..). \n",
        "\n",
        "Given a sequence of words, the model trained on our dataset will predict the next most probable word. We will call the model repeatedly to generate longer sequences.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYTDXoAXXQaH",
        "colab_type": "text"
      },
      "source": [
        "##Setup\n",
        "\n",
        "####Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPl_k7f8qANu",
        "colab_type": "code",
        "outputId": "d5e82155-7eb3-4c89-a0b6-3fd604b93a05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW8XEFpPaG6t",
        "colab_type": "text"
      },
      "source": [
        "####Import Keras and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ENSoJKsaRGM",
        "colab_type": "code",
        "outputId": "c6151efe-a91b-42f8-8c96-84b78f04aae4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout,SimpleRNN, LSTM, Embedding\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import LambdaCallback, EarlyStopping\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.utils import shuffle\n",
        "from glob import glob\n",
        "\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1SCfRbkb2Fw",
        "colab_type": "text"
      },
      "source": [
        "####Reading the data\n",
        "\n",
        "We load the Shakespeare text file and take a look at a part of the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0s5PSeNDtHkl",
        "colab_type": "code",
        "outputId": "a27b13f5-de40-471e-bb71-2f8019369ae3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "with open('/content/drive/My Drive/datasets/shakespeare.txt', encoding='utf-8') as f:\n",
        "   story = f.readlines()\n",
        "   \n",
        "print(''.join(story[:20]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0T95x2Z3ftl",
        "colab_type": "text"
      },
      "source": [
        "##Process the Text\n",
        "Before training the model, we need to process the text in a form that is interpretable by the model.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHCR_I-Web-j",
        "colab_type": "text"
      },
      "source": [
        "#### Story in words\n",
        "\n",
        "First, we convert the story data into chunks of words or tokens. Since we want our model to also recognize puntuations as words, we use [replace()](https://docs.python.org/2/library/string.html#string.replace) to add white spaces around them and then use [split()](https://docs.python.org/2/library/stdtypes.html#str.split) to split the story into word chunks. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEz83f1rcm7c",
        "colab_type": "code",
        "outputId": "e6223f0a-e652-43c4-f48d-c88619c996da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "story_in_words = []\n",
        "for i, line in enumerate(story):\n",
        "  story[i] = line.lower().replace('.', ' . ').\\\n",
        "                          replace(',', ' , ').\\\n",
        "                          replace('?', ' ? ').\\\n",
        "                          replace('\"', ' \" ').\\\n",
        "                          replace('!', ' ! ').\\\n",
        "                          replace(':', ' : ').\\\n",
        "                          replace(';', ' ; ').\\\n",
        "                          replace('--', ' ').\\\n",
        "                          replace('-', ' ').\\\n",
        "                          replace(',', ' , ')\n",
        "  story_in_words.extend(story[i].split(' '))\n",
        "\n",
        "print(\"Total number of words in story: %d\" % len(story_in_words))\n",
        "print(\"Unique words in story: %d\" %len(set(story_in_words)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of words in story: 343653\n",
            "Unique words in story: 14833\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtYVe_amqAOP",
        "colab_type": "text"
      },
      "source": [
        "####Creating sequences\n",
        "\n",
        "The next step is to split the entire text into sequences of a certain length. We specify this length by using the *hyper parameter* SEQ_LEN. Sequence length is the number of words that the generative model would take as input to predict the next word. \n",
        "\n",
        "We go over the story by, shifting by one word at each step and take SEQ_LEN + 1 words in a sequence at a time.\n",
        "\n",
        "\n",
        "\n",
        "NOTE: You can play around with different values of the SEQ_LEN hyper parameter to investigate how it affects the performance of your model. This method of trying out different values of hyper parameters in order to find an optimal set for a learning algorithm is called *Hyperparameter Tuning*. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SNEyBCWqwgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEQ_LEN = 10\n",
        "step = 1\n",
        "sentences = []\n",
        "for i in range(0, len(story_in_words) - SEQ_LEN, step):\n",
        "  sentences.append(story_in_words[i : i + SEQ_LEN + 1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UnyuPcQqxcc",
        "colab_type": "text"
      },
      "source": [
        "####Tokenizer\n",
        "\n",
        "Now we need to map the strings to numeric representation. We want each unique word to be represented by a unique integer number. We use the [Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) class by Keras for this task. The class then calls a function that fits the tokenizer on the our sequences of text and builds an internal vocabulary. \n",
        "\n",
        "Note: 0 is a reserved index in this class that won't be assigned to any word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKKGz-x0qR3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)  # transforms each text sequence into a sequence of integer, where each integer represents a unique word\n",
        "sequences = np.asarray(sequences)                    # converting a list to a numpy array\n",
        "vocab = tokenizer.word_counts                        # Dict object of vocabulary with frequency count\n",
        "\n",
        "VOCAB_LEN = len(tokenizer.word_counts) + 1\n",
        "total = sequences.shape[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwGJ7BD_dDMF",
        "colab_type": "text"
      },
      "source": [
        "We take an example of a sentance and see how the Tokenizer class transforms it into a sequence of integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt5WuYNCbVdC",
        "colab_type": "code",
        "outputId": "d340039b-2d8d-45a9-805b-e946dd726910",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "temp = 'You are all resolved rather to die than to famish ?'\n",
        "\n",
        "tokens = tokenizer.texts_to_sequences([temp.split()])\n",
        "print(\"Numeric representation of the sentence: 'You are all resolved rather to die than to famish ?' is --> \", tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numeric representation of the sentence: 'You are all resolved rather to die than to famish ?' is -->  [[13, 48, 41, 1535, 380, 8, 198, 71, 8, 3461, 16]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plEF3tEv0I3f",
        "colab_type": "text"
      },
      "source": [
        "####Training and Test data split\n",
        "\n",
        "Data is shuffled and split into training and test dataset with 15% of the data in the test split. \n",
        "\n",
        "The first *SEQ_LEN* words of the sequence make the input data and the remaining last word is the target data or the next probable word that the model should output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BiZmqykxBLs",
        "colab_type": "code",
        "outputId": "6f32b159-014d-43da-f882-7e6220599b67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "sequences = shuffle(sequences)\n",
        "\n",
        "train_input = sequences[int(total * 0.15):, :-1]\n",
        "train_output = sequences[int(total * 0.15):, -1]\n",
        "\n",
        "test_input = sequences[:int(total * 0.15), :-1]\n",
        "test_output = sequences[:int(total * 0.15), -1]\n",
        "\n",
        "\n",
        "print(\"Input Training Data Shape:\", train_input.shape)\n",
        "print(\"Target Training Data Shape:\", train_output.shape)\n",
        "\n",
        "print(\"Input Test Data Shape:\", test_input.shape)\n",
        "print(\"Target Test Data Shape:\", test_output.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Training Data Shape: (292097, 10)\n",
            "Target Training Data Shape: (292097,)\n",
            "Input Test Data Shape: (51546, 10)\n",
            "Target Test Data Shape: (51546,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVDVyfD00aqt",
        "colab_type": "text"
      },
      "source": [
        "####Build the RNN Model\n",
        "\n",
        "We use the [Keras Sequential Model](https://keras.io/getting-started/sequential-model-guide/) to define the model. To build our simple RNN text generation model, we use four keras layers:\n",
        "\n",
        "\n",
        "*   [keras.layers.Embedding](https://keras.io/layers/embeddings/): used to train a dense representation of words and their relative meanings.\n",
        "*   [keras.layers.SimpleRNN](https://keras.io/layers/recurrent/#simplernn): fully connected RNN layer where the output is fed back to the input.\n",
        "*   [keras.layers.Dropout](https://keras.io/layers/core/#dropout): applies a regularization technique where randomly selected neurons are ignored or \"dropped-out\" during training.\n",
        "*   [keras.layers.Dense](https://keras.io/layers/core/#dense): regular densely connected neural network layer with output size equal to the vocabulary size (number of unique words). This layer is added at the end and uses the softmax activation to output the probablities (that add up to one) for each word. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PMc52m3Afhg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(VOCAB_LEN, 256, input_length=SEQ_LEN))\n",
        "model.add(SimpleRNN(128, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(SimpleRNN(64))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(VOCAB_LEN, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fci7X5txCaKA",
        "colab_type": "text"
      },
      "source": [
        "We compile the model to configure it for training and use categorical crossentropy as our loss function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkNBch7EAi1g",
        "colab_type": "code",
        "outputId": "3d1c20c1-ef5f-43ec-c924-eb5d8017e183",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 10, 256)           3797504   \n",
            "_________________________________________________________________\n",
            "simple_rnn_1 (SimpleRNN)     (None, 10, 128)           49280     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 10, 128)           0         \n",
            "_________________________________________________________________\n",
            "simple_rnn_2 (SimpleRNN)     (None, 64)                12352     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 14834)             964210    \n",
            "=================================================================\n",
            "Total params: 4,823,346\n",
            "Trainable params: 4,823,346\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2c9sr7U0p1w",
        "colab_type": "text"
      },
      "source": [
        "###Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hOylH2UDxXR",
        "colab_type": "text"
      },
      "source": [
        "We create a generator function that generates data batch-by-batch. The generator is run in parallel to the model, for efficiency. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAMVl9Bexap2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator(sent, word, batch_size):\n",
        "\n",
        "  global index\n",
        "  index = 0\n",
        "  while True:\n",
        "    x = np.zeros((batch_size, SEQ_LEN), dtype=np.int)\n",
        "    y = np.zeros((batch_size, VOCAB_LEN), dtype=np.bool)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "      x[i] = sent[index % len(sent)]\n",
        "      y[i] = to_categorical(word[index % len(word)], num_classes=VOCAB_LEN) #convert integers to one-hot encoded vectors\n",
        "      index = index + 1\n",
        "    yield x,y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0vz5ARoEaxT",
        "colab_type": "text"
      },
      "source": [
        "This function samples an index from a softmax probablity array based on the temperature. This technique is called temperature sampling and is used to improve the quality of samples from language models.\n",
        "\n",
        "Note: The high temperature sample displays greater linguistic variety, but the low temperature sample is more grammatically correct. Lowering the temperature allows you to focus on higher probability output sequences and smooth over deficiencies of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoYihS3CAqru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.flip(np.argsort(probas))[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBLJ2ljjGbp-",
        "colab_type": "text"
      },
      "source": [
        "We make a simple display function that prints the stories "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEgcYSEjKqV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display(story):\n",
        "  full = ''\n",
        "  for word in story:\n",
        "    if word == '|newline|':\n",
        "      full = full + '\\n'\n",
        "    elif word in ',.;:?!':\n",
        "      full = full + word\n",
        "    else:\n",
        "      full = full + ' ' + word\n",
        "  \n",
        "  print(full)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIcVgS7U0u1p",
        "colab_type": "text"
      },
      "source": [
        "####Configure Checkpoints\n",
        "\n",
        "We use two types of checkpoints for our model:\n",
        "\n",
        "\n",
        "*   [ModelCheckpoint](https://keras.io/callbacks/#modelcheckpoint): to ensure that checkpoints are saved during training by monitoring a quality (validation accuracy in this case)\n",
        "*   [EarlyStopping](https://keras.io/callbacks/#EarlyStopping): stops training when the monitored quality (validation accuracy) has not improved for a certain number of epochs. This threshold is set by the *patience* argument.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qCZ5JGuAwCx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE =256\n",
        "file_path = \"/content/drive/My Drive/checkpoints/Shakespeare-RNN-epoch{epoch:03d}-words%d-sequence%d-batchsize%d-\" \\\n",
        "            \"loss{loss:.4f}-acc{acc:.4f}-val_loss{val_loss:.4f}-val_acc{val_acc:.4f}.hdf5\" % \\\n",
        "            (VOCAB_LEN, SEQ_LEN, BATCH_SIZE)\n",
        "\n",
        "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', save_best_only=True) # latest best model according to the val_acc monitored will not be overwritten\n",
        "early_stopping = EarlyStopping(monitor='val_acc', patience=20)\n",
        "callbacks_list = [checkpoint, early_stopping]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tADs8aV8025R",
        "colab_type": "text"
      },
      "source": [
        "####Train the Model\n",
        "\n",
        "The model is trained for 30 epochs, but training could stop early due to the callback. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9m5tcGD2A01J",
        "colab_type": "code",
        "outputId": "3ffe1b81-b09c-4453-abb2-48bdbf944973",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit_generator(generator(train_input, train_output, BATCH_SIZE),\n",
        "                    steps_per_epoch=int(len(train_input)/BATCH_SIZE) + 1,\n",
        "                    epochs=30,\n",
        "                    callbacks=callbacks_list,\n",
        "                    validation_data=generator(test_input, test_output, BATCH_SIZE),\n",
        "                    validation_steps=int(len(test_input)/BATCH_SIZE) + 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "1142/1142 [==============================] - 45s 40ms/step - loss: 5.2329 - acc: 0.2073 - val_loss: 4.4871 - val_acc: 0.3290\n",
            "Epoch 2/100\n",
            "1142/1142 [==============================] - 40s 35ms/step - loss: 4.3360 - acc: 0.3302 - val_loss: 4.2293 - val_acc: 0.3471\n",
            "Epoch 3/100\n",
            "1142/1142 [==============================] - 40s 35ms/step - loss: 4.1169 - acc: 0.3459 - val_loss: 4.1671 - val_acc: 0.3505\n",
            "Epoch 4/100\n",
            "1142/1142 [==============================] - 40s 35ms/step - loss: 3.9842 - acc: 0.3571 - val_loss: 4.1491 - val_acc: 0.3548\n",
            "Epoch 5/100\n",
            "1142/1142 [==============================] - 40s 35ms/step - loss: 3.8811 - acc: 0.3680 - val_loss: 4.1161 - val_acc: 0.3593\n",
            "Epoch 6/100\n",
            "1142/1142 [==============================] - 40s 35ms/step - loss: 3.7906 - acc: 0.3759 - val_loss: 4.1247 - val_acc: 0.3583\n",
            "Epoch 7/100\n",
            "1142/1142 [==============================] - 40s 35ms/step - loss: 3.7169 - acc: 0.3828 - val_loss: 4.1323 - val_acc: 0.3578\n",
            "Epoch 8/100\n",
            "1142/1142 [==============================] - 40s 35ms/step - loss: 3.6448 - acc: 0.3893 - val_loss: 4.1227 - val_acc: 0.3583\n",
            "Epoch 9/100\n",
            "1142/1142 [==============================] - 39s 35ms/step - loss: 3.5839 - acc: 0.3938 - val_loss: 4.1314 - val_acc: 0.3586\n",
            "Epoch 10/100\n",
            "1142/1142 [==============================] - 39s 35ms/step - loss: 3.5255 - acc: 0.3988 - val_loss: 4.1631 - val_acc: 0.3570\n",
            "Epoch 11/100\n",
            "1142/1142 [==============================] - 40s 35ms/step - loss: 3.4760 - acc: 0.4022 - val_loss: 4.1718 - val_acc: 0.3549\n",
            "Epoch 12/100\n",
            "1142/1142 [==============================] - 39s 35ms/step - loss: 3.4271 - acc: 0.4063 - val_loss: 4.2012 - val_acc: 0.3562\n",
            "Epoch 13/100\n",
            "1142/1142 [==============================] - 40s 35ms/step - loss: 3.3820 - acc: 0.4092 - val_loss: 4.2107 - val_acc: 0.3574\n",
            "Epoch 14/100\n",
            "1142/1142 [==============================] - 40s 35ms/step - loss: 3.3372 - acc: 0.4138 - val_loss: 4.2112 - val_acc: 0.3580\n",
            "Epoch 15/100\n",
            "1142/1142 [==============================] - 40s 35ms/step - loss: 3.2978 - acc: 0.4168 - val_loss: 4.2751 - val_acc: 0.3523\n",
            "Epoch 16/100\n",
            "1142/1142 [==============================] - 39s 35ms/step - loss: 3.2596 - acc: 0.4208 - val_loss: 4.2886 - val_acc: 0.3517\n",
            "Epoch 17/100\n",
            "1142/1142 [==============================] - 40s 35ms/step - loss: 3.2255 - acc: 0.4238 - val_loss: 4.2857 - val_acc: 0.3514\n",
            "Epoch 18/100\n",
            "1142/1142 [==============================] - 40s 35ms/step - loss: 3.1895 - acc: 0.4269 - val_loss: 4.3096 - val_acc: 0.3511\n",
            "Epoch 19/100\n",
            "1142/1142 [==============================] - 40s 35ms/step - loss: 3.1574 - acc: 0.4306 - val_loss: 4.3103 - val_acc: 0.3520\n",
            "Epoch 20/100\n",
            "1142/1142 [==============================] - 39s 35ms/step - loss: 3.1268 - acc: 0.4331 - val_loss: 4.3433 - val_acc: 0.3469\n",
            "Epoch 21/100\n",
            "1142/1142 [==============================] - 40s 35ms/step - loss: 3.1004 - acc: 0.4357 - val_loss: 4.3754 - val_acc: 0.3460\n",
            "Epoch 22/100\n",
            "1142/1142 [==============================] - 40s 35ms/step - loss: 3.0677 - acc: 0.4395 - val_loss: 4.3813 - val_acc: 0.3451\n",
            "Epoch 23/100\n",
            "1142/1142 [==============================] - 40s 35ms/step - loss: 3.0449 - acc: 0.4417 - val_loss: 4.4063 - val_acc: 0.3468\n",
            "Epoch 24/100\n",
            "1142/1142 [==============================] - 40s 35ms/step - loss: 3.0203 - acc: 0.4441 - val_loss: 4.4233 - val_acc: 0.3455\n",
            "Epoch 25/100\n",
            "1142/1142 [==============================] - 40s 35ms/step - loss: 2.9953 - acc: 0.4472 - val_loss: 4.4223 - val_acc: 0.3482\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7faa370977f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N69K3rsr08qE",
        "colab_type": "text"
      },
      "source": [
        "###Generate Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIiRYkS-1XiM",
        "colab_type": "text"
      },
      "source": [
        "####Restore the latest checkpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vy6b74c1Ihz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights('/content/drive/My Drive/Colab Notebooks/Shakespeare-epoch001-words14834-sequence10-loss3.9755-acc0.3642-val_loss4.1913-val_acc0.3557.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qCTRit71AOg",
        "colab_type": "text"
      },
      "source": [
        "####Conditional Samples\n",
        "\n",
        "Here, the story is generated word-by-word, based on a prompt provided. If the number of words in the prompt exceed SEQ_LEN, then the prompt is truncated from the beginning to fit the sequence length. If the length of prompt is less than SEQ_LEN, then zeros are padded in the beginning. The whole story is genrated by predicting the next probable word in a loop.\n",
        "\n",
        "Note: Since 0 is used to pad sequences, it is important that the Tokenizer does not use 0 as an index. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Lc3_y34AtyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_cond_samples(no_of_words, temp):\n",
        "    \n",
        "    print('Enter prompt: \\n')\n",
        "    seed = input()\n",
        "    sentence = tokenizer.texts_to_sequences([seed])[0]\n",
        "\n",
        "    sentence = list(pad_sequences([sentence], maxlen=SEQ_LEN, padding='pre', truncating='pre')[0])\n",
        "\n",
        "\n",
        "    gen_story = []\n",
        "    gen_story.extend(seed.split())\n",
        "\n",
        "    for i in range(no_of_words):\n",
        "        x_pred = np.expand_dims(sentence, axis=0) \n",
        "            \n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_indices = sample(preds, temp)\n",
        "\n",
        "        for ix in next_indices:\n",
        "          if ix == 0:\n",
        "            continue\n",
        "          else:\n",
        "            next_word = tokenizer.index_word[ix]\n",
        "            sentence = sentence[1:]\n",
        "            sentence.append(ix)\n",
        "            break\n",
        "\n",
        "        gen_story.append(next_word)\n",
        "        \n",
        "    display(gen_story)\n",
        "        \n",
        "       \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISZfU2osSEvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generate_cond_samples(no_of_words = 500, temp=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjTYRruE1C6k",
        "colab_type": "text"
      },
      "source": [
        "####Unconditional Samples\n",
        "\n",
        "Here, the story is generated word-by-word by starting with a random seed. One integer is randomly sampled from the index of words and story is generated by prediciting the next probable work in a loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFs9KdtS0Zjs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_uncond_samples(no_of_words, temp):\n",
        "    np.random.seed(0)\n",
        "    seed = np.random.randint(1, VOCAB_LEN, size=SEQ_LEN)\n",
        "\n",
        "    sentence = seed\n",
        "\n",
        "    sentence = list(pad_sequences([sentence], maxlen=SEQ_LEN, padding='pre')[0])\n",
        "\n",
        "    gen_story = []\n",
        "\n",
        "    gen_story.extend(tokenizer.index_word[w] for w in seed)\n",
        "    end_flag = 0\n",
        "\n",
        "    for i in range(no_of_words):\n",
        "        x_pred = np.expand_dims(sentence, axis=0)\n",
        "\n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_indices = sample(preds, temp)\n",
        "\n",
        "        for ix in next_indices:\n",
        "            if ix == 0:\n",
        "                continue\n",
        "\n",
        "            elif '|endofstory|' in tokenizer.word_index.keys():\n",
        "                if ix == tokenizer.word_index['|endofstory|'] :\n",
        "                    end_flag = 1\n",
        "                    break\n",
        "            else:\n",
        "                next_word = tokenizer.index_word[ix]\n",
        "                sentence = sentence[1:]\n",
        "                sentence.append(ix)\n",
        "                break\n",
        "\n",
        "        if end_flag == 1:\n",
        "            break\n",
        "\n",
        "\n",
        "        gen_story.append(next_word)\n",
        "\n",
        "    display(gen_story)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRZkP_XDPR2c",
        "colab_type": "code",
        "outputId": "cd5d9159-52cb-4e39-cbfe-ad446cbad469",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "generate_uncond_samples(no_of_words = 500, temp=0.5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " conclude idles compromise\n",
            " watery edge\n",
            " greets presumes majestical fidiused magistrate thrive, i will not be fast. \n",
            " \n",
            " duke vincentio: \n",
            " i have no more than all a cup of wine. \n",
            " \n",
            " leontes: \n",
            " what, sir, i am so, my lord, and i am dead. \n",
            " \n",
            " duke vincentio: \n",
            " i have been a thousand: but, and my life, and not i turn. \n",
            " \n",
            " angelo: \n",
            " ay, my lord, and let her kill my heart. \n",
            " \n",
            " petruchio: \n",
            " o, i will not be so. \n",
            " \n",
            " sicinius: \n",
            " i am not so. \n",
            " \n",
            " nurse: \n",
            " no, what you say, how i am, is gone. \n",
            " \n",
            " romeo: \n",
            " i am the duke of york, but i will see't to chide him; \n",
            " and i will be so. \n",
            " \n",
            " sebastian: \n",
            " i have no less than a word of. \n",
            " what is't, a king, i'll lay it from him, and the settled son and sanctimonious journey, \n",
            " let me be not in this heads. \n",
            " \n",
            " hastings: \n",
            " what, thou art not so. \n",
            " \n",
            " first watchman: \n",
            " he was a man in a holy tale: \n",
            " and let the use of the complexion of the vessel\n",
            " of its scraping back, and i come to me. \n",
            " \n",
            " katharina: \n",
            " nay, sir, i have not a fool, \n",
            " and the king's father will be something than myself, \n",
            " and i will be a fool, and i have not a man. \n",
            " \n",
            " petruchio: \n",
            " why, sir, i am not so. \n",
            " \n",
            " duke vincentio: \n",
            " i have in the manner to the story of his crown, \n",
            " and bring me to a man. \n",
            " \n",
            " polixenes: \n",
            " i pray you, what is this, and i will go to my daughter; but\n",
            " that will be a snapper with strokes and dark. \n",
            " \n",
            " hastings: \n",
            " i am good for a great good man; and, and the sepulchre, \n",
            " we should be none to be a man. \n",
            " \n",
            " gloucester: \n",
            " no, madam, i\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiFZx-rBTEPR",
        "colab_type": "text"
      },
      "source": [
        "Looking at the generated text, you'll see the model knows when to use punctuations, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCEcBb9hZOPj",
        "colab_type": "text"
      },
      "source": [
        "###Build an LSTM Model\n",
        "\n",
        "We keep the structure for the model similar but use the Keras LSTM layer instead of the SimpleRNN layer. We again use the [Keras Sequential Model](https://keras.io/getting-started/sequential-model-guide/) to define the LSTM based generative model by using four keras layers:\n",
        "\n",
        "\n",
        "*   [keras.layers.Embedding](https://keras.io/layers/embeddings/): used to train a dense representation of words and their relative meanings.\n",
        "*   [keras.layers.LSTM](https://keras.io/layers/recurrent/#lstm): long-short term memory layer composed of a *cell*, an *input gate*, an *output gate* and a *forget gate*.\n",
        "*   [keras.layers.Dropout](https://keras.io/layers/core/#dropout): applies a regularization technique where randomly selected neurons are ignored or \"dropped-out\" during training.\n",
        "*   [keras.layers.Dense](https://keras.io/layers/core/#dense): regular densely connected neural network layer with output size equal to the vocabulary size (number of unique words). This layer is added at the end and uses the softmax activation to output the probablities (that add up to one) for each word. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuF2hSk_2p8d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_model = Sequential()\n",
        "lstm_model.add(Embedding(VOCAB_LEN, 256, input_length=SEQ_LEN))\n",
        "lstm_model.add(LSTM(128, return_sequences=True))\n",
        "lstm_model.add(Dropout(0.2))\n",
        "lstm_model.add(LSTM(64))\n",
        "lstm_model.add(Dropout(0.2))\n",
        "lstm_model.add(Dense(VOCAB_LEN, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m70vEiAEg5tc",
        "colab_type": "text"
      },
      "source": [
        "We compile the model to configure it for training and use categorical crossentropy as our loss function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvQm0ofZ2q13",
        "colab_type": "code",
        "outputId": "63e06ebb-d010-43b9-98b2-7ba9bb2b5663",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "source": [
        "lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "lstm_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 10, 256)           5127936   \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 10, 128)           197120    \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 10, 128)           0         \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 64)                49408     \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 20031)             1302015   \n",
            "=================================================================\n",
            "Total params: 6,676,479\n",
            "Trainable params: 6,676,479\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orIlNcpMhAW9",
        "colab_type": "text"
      },
      "source": [
        "### Configure Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7E-18Ssn2_wr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE =256\n",
        "file_path = \"/content/drive/My Drive/checkpoints/Shakespeare-LSTM-epoch{epoch:03d}-words%d-sequence%d-batchsize%d-\" \\\n",
        "            \"loss{loss:.4f}-acc{acc:.4f}-val_loss{val_loss:.4f}-val_acc{val_acc:.4f}.hdf5\" % \\\n",
        "            (VOCAB_LEN, SEQ_LEN, BATCH_SIZE)\n",
        "\n",
        "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', save_best_only=True)\n",
        "early_stopping = EarlyStopping(monitor='val_acc', patience=10)\n",
        "callbacks_list = [checkpoint, early_stopping]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0ZBBSeghFB8",
        "colab_type": "text"
      },
      "source": [
        "###Train the LSTM Model\n",
        "\n",
        "The model is trained for 30 epochs, but training could stop early due to the callback. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YokKvI_X2zeQ",
        "colab_type": "code",
        "outputId": "0f80b6c1-d9d1-43ce-c6d2-1bef2aaf893a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "lstm_model.fit_generator(generator(train_input, train_output, BATCH_SIZE),\n",
        "                    steps_per_epoch=int(len(train_input)/BATCH_SIZE) + 1,\n",
        "                    epochs=30,\n",
        "                    callbacks=callbacks_list,\n",
        "                    validation_data=generator(test_input, test_output, BATCH_SIZE),\n",
        "                    validation_steps=int(len(test_input)/BATCH_SIZE) + 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "2967/2967 [==============================] - 193s 65ms/step - loss: 4.7060 - acc: 0.3779 - val_loss: 4.6633 - val_acc: 0.3739\n",
            "Epoch 2/100\n",
            "2967/2967 [==============================] - 193s 65ms/step - loss: 4.6286 - acc: 0.3780 - val_loss: 4.6606 - val_acc: 0.3736\n",
            "Epoch 3/100\n",
            "2967/2967 [==============================] - 192s 65ms/step - loss: 4.1183 - acc: 0.4032 - val_loss: 3.6000 - val_acc: 0.4620\n",
            "Epoch 4/100\n",
            "2967/2967 [==============================] - 191s 64ms/step - loss: 3.4038 - acc: 0.4760 - val_loss: 3.3236 - val_acc: 0.4872\n",
            "Epoch 5/100\n",
            "2967/2967 [==============================] - 191s 64ms/step - loss: 3.1931 - acc: 0.4954 - val_loss: 3.2089 - val_acc: 0.4966\n",
            "Epoch 6/100\n",
            "2967/2967 [==============================] - 191s 64ms/step - loss: 3.0667 - acc: 0.5052 - val_loss: 3.1520 - val_acc: 0.5016\n",
            "Epoch 7/100\n",
            "2967/2967 [==============================] - 192s 65ms/step - loss: 2.9717 - acc: 0.5126 - val_loss: 3.1142 - val_acc: 0.5065\n",
            "Epoch 8/100\n",
            "2967/2967 [==============================] - 195s 66ms/step - loss: 2.8984 - acc: 0.5179 - val_loss: 3.0980 - val_acc: 0.5084\n",
            "Epoch 9/100\n",
            "2967/2967 [==============================] - 190s 64ms/step - loss: 2.8413 - acc: 0.5220 - val_loss: 3.0879 - val_acc: 0.5101\n",
            "Epoch 10/100\n",
            "2967/2967 [==============================] - 190s 64ms/step - loss: 2.7914 - acc: 0.5262 - val_loss: 3.0779 - val_acc: 0.5121\n",
            "Epoch 11/100\n",
            "2967/2967 [==============================] - 190s 64ms/step - loss: 2.7479 - acc: 0.5293 - val_loss: 3.0772 - val_acc: 0.5121\n",
            "Epoch 12/100\n",
            "2967/2967 [==============================] - 191s 64ms/step - loss: 2.7096 - acc: 0.5324 - val_loss: 3.0833 - val_acc: 0.5120\n",
            "Epoch 13/100\n",
            "2967/2967 [==============================] - 191s 64ms/step - loss: 2.6753 - acc: 0.5353 - val_loss: 3.0843 - val_acc: 0.5144\n",
            "Epoch 14/100\n",
            "2967/2967 [==============================] - 191s 64ms/step - loss: 2.6432 - acc: 0.5378 - val_loss: 3.0945 - val_acc: 0.5134\n",
            "Epoch 15/100\n",
            "2967/2967 [==============================] - 190s 64ms/step - loss: 2.6166 - acc: 0.5401 - val_loss: 3.1055 - val_acc: 0.5117\n",
            "Epoch 16/100\n",
            "2967/2967 [==============================] - 190s 64ms/step - loss: 2.5880 - acc: 0.5430 - val_loss: 3.1112 - val_acc: 0.5134\n",
            "Epoch 17/100\n",
            "2967/2967 [==============================] - 194s 65ms/step - loss: 2.5626 - acc: 0.5450 - val_loss: 3.1292 - val_acc: 0.5134\n",
            "Epoch 18/100\n",
            "2967/2967 [==============================] - 191s 64ms/step - loss: 2.5376 - acc: 0.5475 - val_loss: 3.1352 - val_acc: 0.5124\n",
            "Epoch 19/100\n",
            "2967/2967 [==============================] - 189s 64ms/step - loss: 2.5159 - acc: 0.5488 - val_loss: 3.1421 - val_acc: 0.5134\n",
            "Epoch 20/100\n",
            "2657/2967 [=========================>....] - ETA: 18s - loss: 2.4916 - acc: 0.5518Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJfu6wIMhROr",
        "colab_type": "text"
      },
      "source": [
        "###Generate Text "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC83iYZThV7K",
        "colab_type": "text"
      },
      "source": [
        "####Restore the last checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxITpYOrhZa-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights('/content/drive/My Drive/Colab Notebooks/Sherlock-epoch001-words14834-sequence10-loss3.9755-acc0.3642-val_loss4.1913-val_acc0.3557.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7c-E1Kuhc1H",
        "colab_type": "text"
      },
      "source": [
        "####Unconditional Text Generation\n",
        "\n",
        "Starting from a random seed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmByM7NNZgbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generate_uncond_samples(no_of_words = 500, temp=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9dmYBLzhjMn",
        "colab_type": "text"
      },
      "source": [
        "####Conditional Text Generation\n",
        "\n",
        "Enter a starting prompt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04_xTrmiZjF6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generate_cond_samples(no_of_words = 500, temp=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}